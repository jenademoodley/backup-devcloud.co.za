<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>DevCloud</title>
    <link href="https://devcloud.co.za/feed.xml" rel="self" />
    <link href="https://devcloud.co.za" />
    <updated>2023-12-11T18:42:08+02:00</updated>
    <author>
        <name>Jenade Moodley</name>
    </author>
    <id>https://devcloud.co.za</id>

    <entry>
        <title>How to configure cgroupv2 on Amazon Linux 2</title>
        <author>
            <name>Jenade Moodley</name>
        </author>
        <link href="https://devcloud.co.za/test/"/>
        <id>https://devcloud.co.za/test/</id>
        <media:content url="https://devcloud.co.za/media/posts/22/98dd3b8b34ef4a7c9a8f29738e221408.png" medium="image" />
            <category term="Linux"/>
            <category term="EC2"/>
            <category term="Amazon Linux"/>
            <category term="AWS"/>

        <updated>2023-12-11T18:42:08+02:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://devcloud.co.za/media/posts/22/98dd3b8b34ef4a7c9a8f29738e221408.png" alt="" />
                    Amazon Linux 2 is the current default or vanilla Amazon Linux version currently used for AWS workloads. However, when using this version, one would notice that this OS still uses cgroup version 1. Given that cgroupv2 has been around since October 2015 and that certain&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://devcloud.co.za/media/posts/22/98dd3b8b34ef4a7c9a8f29738e221408.png" class="type:primaryImage" alt="" /></p>
                
  <p>
    Amazon Linux 2 is the current default or vanilla Amazon Linux version currently used for AWS workloads. However, when using this version, one would notice that this OS still uses cgroup version 1. Given that cgroupv2 has been around since October 2015 and that certain functions such as&nbsp;<a href="https://kubernetes.io/blog/2021/11/26/qos-memory-resources/" target="_blank">MemoryQoS in Kubernetes</a> relies on cgroupv2. As such, many users would like to enable cgroupv2 on Amazon Linux 2.
  </p>

    <h2 id="why-is-cgroupv2-not-enabled-on-amazon-linux-2">
      Why is cgroupv2 not enabled on Amazon Linux 2?
    </h2>

  <p>
    The reason for the discrepancy is due to the version of systemd included in Amazon Linux 2. At present, the latest version of systemd available on Amazon Linux 2 is <code>219</code>, however the recommended systemd version&nbsp; is <code>244</code> or later. Older systemd does not support delegation of <code>cpuset</code> controller. This is why Amazon Linux 2 still uses cgroup version 1.
  </p>

  <p>
    However, this also shows us the solution to enable cgroupv2, by updating the version of systemd on Amazon Linux 2.
  </p>

    <h2 id="how-can-i-update-systemd-on-amazon-linux-2">
      How can I update systemd on Amazon Linux 2
    </h2>

  <p>
    At present, version <code>219</code> is the latest version of systemd as mentioned above, and AWS has not provided any indication of when a more up to date version would be released, if at all. As such, we can install an updated version of systemd by installing from source. The steps to install an updated systemd from source (tested on a current version of the Amazon Linux 2 AMI) are as below.
  </p>

  <ol>
    <li>Switch to root user.</li><li>Install kernel version 5.10 or later.</li><li>Download building tools.</li><li>Download and extract systemd source files.<br></li><li>Build and install systemd.</li><li>Rebuild initramfs</li><li>Reboot.</li>
  </ol>

    <h4 id="switch-to-root">
      Switch to root
    </h4>
<pre class="line-numbers  language-bash"><code>$ sudo -i
</code></pre>

    <h4 id="install-kernel-version-510-or-later">
      Install kernel version 5.10 or later.
    </h4>
<pre class="line-numbers  language-bash"><code>$ amazon-linux-extras install kernel-5.10 -y
</code></pre>

  <p>
    Take note of the exact kernel version installed as we will use this in a later command. At the time of writing, I installed kernel version&nbsp;<code>5.10.201-191.748.amzn2.x86_64</code>.
  </p>
<pre class="line-numbers  language-bash"><code>  Installing : kernel-5.10.201-191.748.amzn2.x86_64                                                                     1/1
  Verifying  : kernel-5.10.201-191.748.amzn2.x86_64                                                                     1/1

Installed:
  kernel.x86_64 0:5.10.201-191.748.amzn2</code></pre>

    <h4 id="download-building-tools">
      Download building tools.
    </h4>
<pre class="line-numbers  language-bash"><code>$ yum groupinstall "Development Tools" -y
$ yum install -y libmount-devel libcap-devel gperf glib2-devel python3-pip
$ pip3 install --user meson ninja jinja2
$ export PATH=$PATH:~/.local/bin</code></pre>

    <h4 id="download-and-extract-systemd-source-files">
      Download and extract systemd source files.
    </h4>
<pre class="line-numbers  language-bash"><code>$ curl -OL https://github.com/systemd/systemd/archive/refs/tags/v254.tar.gz
$ tar -xf v254.tar.gz
$ cd systemd-254/</code></pre>

  <p>
    The latest version of systemd is 254 at the time of writing. You can choose to install a different or more up to date version of systemd from <a href="https://github.com/systemd/systemd/releases" target="_blank">systemd's github release page</a>.
  </p>

    <h4 id="build-and-install-systemd">
      Build and install systemd.
    </h4>
<pre class="line-numbers  language-bash"><code>$ ./configure
$ make -j $(nproc --all)
$ make install -j $(nproc --all)
$ systemctl --version</code></pre>

    <h4 id="rebuild-initramfs">
      Rebuild initramfs
    </h4>

  <p>
    The initramfs needs to be rebuilt with the newly installed systemd modules. The modules must be built for the newly installed 5.10 kernel. You can rebuild the initramfs for this kernel using the&nbsp;<code>dracut</code> command, specifying the version of the kernel installed in step 1.
  </p>
<pre class="line-numbers  language-bash"><code>$ dracut -f -v --kver 5.10.201-191.748.amzn2.x86_64</code></pre>

    <h4 id="reboot">
      Reboot
    </h4>

  <p>
    The system must be rebooted in order to load the new kernel.
  </p>
<pre class="line-numbers  language-bash"><code>$ reboot
</code></pre>

  <p>
    After these steps, you would notice that cgroup2 is now enabled. You can confirm if this is the case by using the below command:
  </p>
<pre class="line-numbers  language-bash"><code>$ stat -fc %T /sys/fs/cgroup/
</code></pre>

  <p>
    If the command returns <code>tmpfs</code>, cgroup version 1 is still in effect. However, if the command returns <code>cgroup2fs</code>, cgroupv2 has been successfully enabled, which is the expected output after the above steps.
  </p>

    <h2 id="conclusion">
      Conclusion
    </h2>

  <p>
    The above steps can be used to enable cgroupv2 on Amazon Linux 2, at least until Amazon releases an updated systemd version, or until Amazon Linux 2 is sunset and we move on to the next version. In the meantime however, we will have to settle for manually upgrading systemd to get cgroupv2.&nbsp;
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>How to use ARM-based GPU EC2 instances as ECS container instances</title>
        <author>
            <name>Jenade Moodley</name>
        </author>
        <link href="https://devcloud.co.za/how-to-use-arm-based-gpu-ec2-instances-as-ecs-container-instances/"/>
        <id>https://devcloud.co.za/how-to-use-arm-based-gpu-ec2-instances-as-ecs-container-instances/</id>
        <media:content url="https://devcloud.co.za/media/posts/17/AWS-Nvidia.jpg" medium="image" />
            <category term="NVIDIA"/>
            <category term="GPU"/>
            <category term="ECS"/>
            <category term="EC2"/>
            <category term="Docker"/>
            <category term="ARM64"/>

        <updated>2022-06-01T20:17:11+02:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://devcloud.co.za/media/posts/17/AWS-Nvidia.jpg" alt="" />
                    With new EC2 instance types such as G5g, you can now use GPUs with ARM on EC2. While ECS offers some the ECS-optimized AMI as a way to quickly setup your EC2 instances for ECS workloads, you will find that the GPU optimized AMI is&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://devcloud.co.za/media/posts/17/AWS-Nvidia.jpg" class="type:primaryImage" alt="" /></p>
                <p>With new EC2 instance types such as G5g, you can now use GPUs with ARM on EC2. While ECS offers some the ECS-optimized AMI as a way to quickly setup your EC2 instances for ECS workloads, you will find that the GPU optimized AMI is only for the <code>x86</code> platform, and there is no support for ARM-based GPU instances. The reason for this is quite simple; NVIDIA GPU container workloads require the <a href="https://nvidia.github.io/nvidia-docker/"><code>nvidia-docker</code></a> container runtime, and support is not yet available for Amazon Linux. Support is only available for CentOS 8, RHEL 8, RHEL 9, and Ubuntu versions 18.04 and above. You can review the supported distributions in <a href="https://nvidia.github.io/nvidia-docker/"><code>nvidia-docker</code> documentation</a>.</p>
<p>Given that <a href="https://www.centos.org/centos-linux-eol/">CentOS 8 has reached EOL</a> and <a href="https://www.techrepublic.com/article/a-matter-of-license-and-lock-in/">RedHat has dropped support for the Docker runtime engine</a>, that leaves us only with Ubuntu. You could try and setup Docker on CentOS or RHEL, but given the lack of support this would not be advisable.</p>
<p>To setup an Ubuntu ARM-based GPU instance for ECS, we would need to follow the below steps.</p>
<ol>
<li><a href="#install-nvidia-drivers">Install NVIDIA drivers.</a></li>
<li><a href="#install-docker-and-nvidia-container-runtime">Install Docker and NVIDIA container runtime.</a></li>
<li><a href="#install-ecs-agent">Install ECS agent.</a></li>
</ol>
<p>In my testing I used Ubuntu 20.04 on a G5g.xlarge instance, but these steps should work regardless of your distribution.</p>
<h3 id="install-nvidia-drivers">Install NVIDIA Drivers</h3>
<p>G5g instances currently support the Tesla driver version 470.82.01 or later. You can download the appropriate Driver directly from <a href="http://www.nvidia.com/Download/Find.aspx">Nvidia</a>.</p>
<p>You can use the below specifications to find the driver, <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/install-nvidia-driver.html#nvidia-installation-options">straight from AWS documentation</a></p>
<table>
<thead>
<tr>
<th>Instance</th>
<th>Product Type</th>
<th>Product Series</th>
<th>Product</th>
</tr>
</thead>
<tbody><tr>
<td>G5g</td>
<td>Tesla</td>
<td>T-Series</td>
<td>NVIDIA T4G</td>
</tr>
</tbody></table>
<p>In this example, I am using version “510.73.08” of the driver, the latest version of the driver at the time of writing this article. This driver can be installed with the below steps:</p>
<ol>
<li>Download the driver.</li>
</ol>
<pre><code>$ wget https://us.download.nvidia.com/tesla/510.73.08/NVIDIA-Linux-aarch64-510.73.08.run
</code></pre>
<ol start="2">
<li>Install <code>gcc</code>, <code>make</code> and headers.</li>
</ol>
<pre><code>$ sudo apt-get update -y
$ sudo apt-get install gcc make linux-headers-$(uname -r) -y
</code></pre>
<ol start="3">
<li>Run the executable.</li>
</ol>
<pre><code>$ chmod +x NVIDIA-Linux-aarch64-510.73.08.run
$ sudo sh ./NVIDIA-Linux-aarch64-510.73.08.run  --disable-nouveau --silent
</code></pre>
<ol start="4">
<li>(Optional) Test if GPU is detected.</li>
</ol>
<pre><code>$ nvidia-smi
</code></pre>
<h3 id="install-docker-and-nvidia-container-runtime">Install Docker and NVIDIA container runtime</h3>
<ol>
<li>Download and install Docker.</li>
</ol>
<pre><code>$ curl https://get.docker.com | sh   &amp;&amp; sudo systemctl --now enable docker
</code></pre>
<ol start="2">
<li>Setup NVIDIA repository information.</li>
</ol>
<pre><code>$ distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
      &amp;&amp; curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
      &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
            sed &#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#39; | \
            sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
</code></pre>
<ol start="3">
<li>Install NVIDIA container runtime.</li>
</ol>
<pre><code>$ sudo apt-get update
$ sudo apt-get install -y nvidia-docker2 nvidia-container-runtime
$ sudo systemctl restart docker
</code></pre>
<ol start="4">
<li>Confirm if the NVIDIA runtime is used by Docker.</li>
</ol>
<pre><code>$ sudo docker info --format &#39;{{json .Runtimes.nvidia}}&#39;
</code></pre>
<p>You should receive the below output.</p>
<pre><code>{&quot;path&quot;:&quot;nvidia-container-runtime&quot;}
</code></pre>
<h3 id="install-ecs-agent">Install ECS agent</h3>
<ol>
<li>Download the ARM-based ECS agent for Ubuntu.</li>
</ol>
<pre><code>$ curl -O https://s3.us-east-1.amazonaws.com/amazon-ecs-agent-us-east-1/amazon-ecs-init-latest.arm64.deb
</code></pre>
<ol start="2">
<li>Install the ECS agent.</li>
</ol>
<pre><code>$ sudo dpkg -i amazon-ecs-init-latest.arm64.deb
</code></pre>
<ol start="3">
<li>Configure GPU support for the agent.</li>
</ol>
<pre><code>$ sudo mkdir -p /etc/ecs/
$ sudo touch /etc/ecs/ecs.config
$ echo &quot;ECS_ENABLE_GPU_SUPPORT=true&quot; | sudo tee -a /etc/ecs/ecs.config
</code></pre>
<p>At this stage, you can add any additional configuration to the ecs.config file such as setting the ECS cluster.</p>
<ol start="4">
<li>Start the agent.</li>
</ol>
<pre><code>$ sudo systemctl enable ecs
$ sudo systemctl start ecs
</code></pre>
<p>And that’s it! You would see the instance in the ECS console, registered as a container instance. You can begin assigning GPUs to your containers and schedule them on these instances. If you would like to test a sample application, you can use the below ECS task definition to simply check the NVIDIA GPU with <code>nvidia-smi</code>.</p>
<pre><code>    {
        &quot;containerDefinitions&quot;: [
            {
            &quot;memory&quot;: 80,
            &quot;essential&quot;: true,
            &quot;name&quot;: &quot;gpu&quot;,
            &quot;image&quot;: &quot;nvidia/cuda:11.4.0-base-ubuntu20.04&quot;,
            &quot;resourceRequirements&quot;: [
                {
                &quot;type&quot;:&quot;GPU&quot;,
                &quot;value&quot;: &quot;1&quot;
                }
            ],
            &quot;command&quot;: [
                &quot;sh&quot;,
                &quot;-c&quot;,
                &quot;nvidia-smi&quot;
            ],
            &quot;cpu&quot;: 100
            }
        ],
        &quot;family&quot;: &quot;example-ecs-gpu&quot;
    }
</code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>These steps are necessary in order to setup Ubuntu for ECS workloads on G5g instances. There may be other ARM-based GPU instances added, but this looks to be the only one for now. We can expect support for Amazon Linux once NVIDIA adds support, but there’s no confirmation when or even if that will be. It would also be nice to see additional OS support such as Rocky Linux to allow for more variety, but this ultimately depends on NVIDIA. Only time will tell what else we can use. For now, this is a working solution which you can use to setup your ECS workloads on ARM-based GPU EC2 instances.</p>

            ]]>
        </content>
    </entry>
    <entry>
        <title>Diagnosing EC2 Performance Issues</title>
        <author>
            <name>Jenade Moodley</name>
        </author>
        <link href="https://devcloud.co.za/diagnosing-ec2-performance-issues/"/>
        <id>https://devcloud.co.za/diagnosing-ec2-performance-issues/</id>
        <media:content url="https://devcloud.co.za/media/posts/15/How-Fast-or-Slow-Are-Turtles.jpg" medium="image" />
            <category term="EC2"/>

        <updated>2022-02-11T00:01:05+02:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://devcloud.co.za/media/posts/15/How-Fast-or-Slow-Are-Turtles.jpg" alt="" />
                    If you are an AWS beginner, or even an avid AWS user, you may experience times when your EC2 instance grinds to a halt for no apparent reason. This impact to performance is because you hit a limit, and are now being throttled. In any&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://devcloud.co.za/media/posts/15/How-Fast-or-Slow-Are-Turtles.jpg" class="type:primaryImage" alt="" /></p>
                
  <p>
    If you are an AWS beginner, or even an avid AWS user, you may experience times when your EC2 instance grinds to a halt for no apparent reason. This impact to performance is because you hit a limit, and are now being throttled. In any server, there are four resources you need to monitor:
  </p>

  <ol>
    <li>CPU</li><li>Memory (RAM)</li><li>Disk I/O</li><li>Network I/O</li>
  </ol>

  <p>
    If even one of these hit a limit, you will experience slow performance on your server. EC2 instances are no different. You need to know how to diagnose which of the above resources are constrained. Luckily, AWS provides CloudWatch metrics you can use to get a quick glance at your instance to determine if you are experiencing any issues.
  </p>

    <h2 id="cloudwatch-metrics-for-your-ec2-instance">
      CloudWatch metrics for your EC2 instance
    </h2>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="1">
      <figure class="gallery__item">
      <a href="https://devcloud.co.za/media/posts/15/gallery/Screenshot-74.png" data-size="1809x559">
        <img loading="lazy" src="https://devcloud.co.za/media/posts/15/gallery/Screenshot-74-thumbnail.png" height="237" width="768" alt="" >
      </a>
      <figcaption>EC2 CloudWatch Metrics</figcaption>
    </figure>
    </div>
  </div>

  <p>
    If you were to select the "Monitoring" tab in your EC2 console, you would see a bunch of metrics related to your EC2 instance. You can see the CPU usage (CPUUtilization, and CPUCreditBalance which is explained further below) and Network usage (NetworkIn, NetworkOut, NetworkPacketsIn, NetworkPacketsOut). A full list of metrics are listed in the <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html" target="_blank">AWS EC2 documentation</a>.&nbsp;
  </p>

  <p>
    However, you may notice that there are DiskReadOps and DiskWriteOps metrics, but these metrics are sometimes empty. This is because these metrics refer to instance storage volumes, and would thus only be populated for instances which are configured to use instance store. The storage used by most instances, and is the root volume for all current generation instance types, are EBS volumes. EBS volumes are decoupled from EC2 instances to ensure availability of your data when there are issues with your instance. As such, you would need to navigate to the EBS console and locate the volumes attached to your instance in order to view your EBS usage.
  </p>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="1">
      <figure class="gallery__item">
      <a href="https://devcloud.co.za/media/posts/15/gallery/Screenshot-76.png" data-size="1765x474">
        <img loading="lazy" src="https://devcloud.co.za/media/posts/15/gallery/Screenshot-76-thumbnail.png" height="206" width="768" alt="" >
      </a>
      <figcaption>EBS CloudWatch Usage</figcaption>
    </figure>
    </div>
  </div>

  <p>
    And finally, there are no metrics for memory usage. AWS does not record memory usage of your EC2 instance by default. In order to record the memory usage, you need to perform the additional step of <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html" target="_blank">setting up the CloudWatch agent to send memory usage to CloudWatch</a>.
  </p>

  <p>
    However, if you can't access the CloudWatch metrics in the console for some reason (maybe you are in an Organization, and you have not been granted access), you can still monitor all of the above metrics from within the EC2 instance. If you are using Linux, you can use the <code>top</code> command to obtain an overview of your system, and on Windows you can use Task Manager.
  </p>

    <h2 id="linux--the-top-command">
      Linux - The top command
    </h2>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://devcloud.co.za/media/posts/15/Screenshot-71.png" height="624" width="1076" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://devcloud.co.za/media/posts/15/responsive/Screenshot-71-xs.png 300w ,https://devcloud.co.za/media/posts/15/responsive/Screenshot-71-sm.png 480w ,https://devcloud.co.za/media/posts/15/responsive/Screenshot-71-md.png 768w ,https://devcloud.co.za/media/posts/15/responsive/Screenshot-71-lg.png 1024w ,https://devcloud.co.za/media/posts/15/responsive/Screenshot-71-xl.png 1360w ,https://devcloud.co.za/media/posts/15/responsive/Screenshot-71-2xl.png 1600w">
      <figcaption>Linux top command output</figcaption>
    </figure>

  <p>
    The top command is an easy way to gain an overview of the system. You can view the CPU and memory usage of your system, as well as per process. There are no readings for disk and network metrics, but we can diagnose if there are any issues with these metrics using the load average. The load average measures the amount of load on your system, and includes processes using CPU, as well as processes waiting to use CPU. Such processes contribute to the <code>iowait</code> metric. As such, if load average is high but CPU usage is low, then that means that your system is suffering from I/O wait. In this case, you would usually see a large number of processes stuck in D state, indicating they are waiting for I/O to complete.
  </p>

  <p>
    However, if you would like to monitor network and disk I/O, you can look at using <a href="https://linux.die.net/man/1/sar"><code>sar</code></a>. <code>sar</code> is a very powerful tool which you can use to view and even record in-depth metrics on your system. <code>sar</code> is part of the <code>sysstat</code> package, hence you can simply install <code>sysstat</code> in order to begin using sar:
  </p>

    <h5 id="amazon-linux-centos-rhel">
      Amazon Linux, CentOS, RHEL
    </h5>
<pre class="line-numbers  language-bash"><code>$ sudo yum install sysstat -y</code></pre>

    <h5 id="ubuntu">
      Ubuntu
    </h5>
<pre class="line-numbers  language-bash"><code>sudo apt-get update
sudo apt-get install sysstat</code></pre>

  <p>
    I won't be going through all of <code>sar</code>, but you can view disk and network usage with the below commands:
  </p>

    <h5 id="disk">
      Disk
    </h5>
<pre class="line-numbers  language-html"><code>$ sar -p -d 1 1
Linux 4.14.256-197.484.amzn2.x86_64 (ip-10-0-138-139.eu-west-1.compute.internal)        02/10/22        _x86_64_        (2 CPU)

17:45:01          DEV       tps  rd_sec/s  wr_sec/s  avgrq-sz  avgqu-sz     await     svctm     %util
17:45:02      nvme0n1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00

Average:          DEV       tps  rd_sec/s  wr_sec/s  avgrq-sz  avgqu-sz     await     svctm     %util
Average:      nvme0n1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</code></pre>

    <h5 id="network">
      Network
    </h5>
<pre class="line-numbers  language-html"><code>$ sar -n DEV 1 1
Linux 4.14.256-197.484.amzn2.x86_64 (ip-10-0-138-139.eu-west-1.compute.internal)        02/10/22        _x86_64_        (2 CPU)

17:45:41        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s
17:45:42         eth0      2.00      0.00      0.09      0.00      0.00      0.00      0.00
17:45:42           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00
17:45:42      docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00
17:45:42         eth1      0.00      0.00      0.00      0.00      0.00      0.00      0.00

Average:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s
Average:         eth0      2.00      0.00      0.09      0.00      0.00      0.00      0.00
Average:           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:      docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:         eth1      0.00      0.00      0.00      0.00      0.00      0.00      0.00</code></pre>

    <h2 id="windows--task-manager">
      Windows - Task Manager
    </h2>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://devcloud.co.za/media/posts/15/Screenshot-2022-02-10-194153.jpg" height="1033" width="1125" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://devcloud.co.za/media/posts/15/responsive/Screenshot-2022-02-10-194153-xs.jpg 300w ,https://devcloud.co.za/media/posts/15/responsive/Screenshot-2022-02-10-194153-sm.jpg 480w ,https://devcloud.co.za/media/posts/15/responsive/Screenshot-2022-02-10-194153-md.jpg 768w ,https://devcloud.co.za/media/posts/15/responsive/Screenshot-2022-02-10-194153-lg.jpg 1024w ,https://devcloud.co.za/media/posts/15/responsive/Screenshot-2022-02-10-194153-xl.jpg 1360w ,https://devcloud.co.za/media/posts/15/responsive/Screenshot-2022-02-10-194153-2xl.jpg 1600w">
      <figcaption>Windows Task Manager Output</figcaption>
    </figure>

  <p>
    The Windows task manager provides a graphical interface for you to view the overall metrics of your system. The graphs are intuitive and easy to understand.
  </p>

    <h2 id="what-do-i-look-for-in-the-monitoring-tools">
      What do I look for in the monitoring tools?
    </h2>

  <p>
    As you would have hit a limit for one of the above resources, you cannot use any more of that resource. Projecting that analogy to a graph, this would be seen by a <u>flat line</u>. A PC or server's usage of resources are not always exactly the same; there would be some slight variance over time. If there is a flat line for any metric, chances are that you are being throttled on that metric.
  </p>

  <p>
    When you notice a flat line, you should check the details for your instance or EBS volume to determine the available limits. For CPU and memory, a value which is consistently at 100% is a sign of CPU and memory pressure. Similarly, Task Manager would also consistently be at 100%. In top, the output for %Cpu would show 0.0 idle indicating that the CPU is completely busy, and there would be no available memory.
  </p>

  <p>
    On the other hand, network performance and EBS performance would have a definite value, not a percentage. If you hit a limit, the CloudWatch metric and Task Manager output would flatline at this value. In top, you would see a high value for load average, as well as a high value for iowait. If using sar, you would see that the values returned over a period of time would be similar in value.
  </p>

  <p>
    However, there may be times when the network, CPU, or EBS performance flatlines, and the values seen are not the advertised limits. In this case, that resource is a burstable resource, and is being throttled on the baseline performance.
  </p>

    <h2 id="the-aws-burst-model">
      The AWS Burst Model
    </h2>

  <p>
    AWS incorporates a burst model when using a vast majority of their services. In an EC2 instance, as it is a VM and is on a shared underlying host with other customers, these limits are in place to ensure that you can go above your baseline limits during high workloads, but the throttle is in place so that other instances on the host are unaffected (this behaviour varies per instance type).&nbsp;
  </p>

  <p>
    To explain the burst model, for a specific resource, you are allocated a baseline performance. The baseline performance is over a set time period, usually per second. However, to account for spikes in workloads, your instance can go above the baseline performance for a short period of time using burst credits. If you sustain this usage, the burst credits would steadily be used up until it is depleted, and then you will be throttled back to the baseline performance. If the usage drops below the baseline performance, burst credits would begin to accumulate once again.
  </p>

  <p>
    To explain by means of an example, the t2.micro is a general performance instance type, with a burstable CPU performance. By default, it has 1 vCPU, and a baseline performance of 10% of that vCPU. As soon as you begin to use more than 10% CPU, CPU burst credits are used. As soon as the credits are exhausted, the instance is throttled to 10% CPU usage.
  </p>

  <p>
    This burstable model is in place for the below resources:
  </p>

  <ul>
    <li>CPU usage for T2, T3, T3a, and T4g instance types. The baseline performance differs between each instance type. These are referred to as <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-credits-baseline-concepts.html" target="_blank">Burstable Performance Instances</a>.</li><li>EBS IOPS usage on <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2" target="_blank">gp2 EBS volumes</a>.</li><li>Network performance on a variety of instance types. This is documented in the <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-network-bandwidth.html" target="_blank">AWS EC2 documentation</a>.</li>
  </ul>

  <p>
    CPU and EBS burst details are documented clearly, and you can measure the amount of burst balance remaining for each resource. However, you cannot view the burst balance for network performance. You can only view the baseline performance per instance type in the above documentation.
  </p>

    <h2 id="how-do-i-proceed-after-identifying-the-throttled-resource">
      How do I proceed after identifying the throttled resource?
    </h2>

  <p>
    The first step would be to try and fix the usage. In Task Manager, you can view which process is using up the specified resource. In top, you can identify which process is using up the most CPU and memory, as well as determine which processes are in D state to diagnose network and disk hungry processes. Once identified, you should examine each process to see if this is expected. You can check the logs for that process if possible to see of there was any unusual occurrences. However, if you notice that the resource was slowly and steadily increasing, odds are that your instance just got busier over time. However, do not just assume this, always try and confirm.
  </p>

  <p>
    If you confirm the usage is expected, you then need to allocate more resources to handle the load. In order to increase the resources, you can look at scaling up, or scaling out. Scaling up refers to upgrading the instance type to one which has more of the specified resource. For example, if you are using a t2.micro, you can update to a t2.medium. Scaling out however means to increase the number of instances. This is useful if you are able to make use of multiple instances simultaneously in your application, for example, by means of a load balancer.&nbsp;
  </p>

  <p>
    In the case of gp2 EBS volumes however, you should instead look at migrating to gp3 volumes. gp3 volumes do not use baseline performance and burst credits. You specify the IOPS and throughput you need, and you will receive consistent performance at those IOPS. g3 volumes are also slightly cheaper than gp2 volumes. For more information, you can refer to the <a href="https://aws.amazon.com/ebs/pricing/" target="_blank">EBS pricing page</a>.
  </p>

    <h2 id="summary">
      Summary
    </h2>

  <p>
    We can recap the above blog with these points:
  </p>

  <ul>
    <li>Your instance can hit a limit on CPU, memory, network, and/or disk which can cause performance issues.&nbsp;</li><li>You can determine which resource is throttled using CloudWatch metrics, or using monitoring tools from within the instance.&nbsp;</li><li>When looking at usage, look for a flatline or similar values in the metrics.&nbsp;</li><li>Don't forget to take into account burstable resources when using certain instance types.</li><li>&nbsp;After identifying the issue, determine if this is expected or an anomaly.</li><li>If expected, you can scale up or scale out, depending on your business needs</li>
  </ul>

  <p>
    This article can help those using EC2 and seeing performance issues, and provides and end-to-end guide of identifying the cause, and fixing the issue.&nbsp;
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>How does the Docker client authenticate to ECR?</title>
        <author>
            <name>Jenade Moodley</name>
        </author>
        <link href="https://devcloud.co.za/how-does-the-docker-client-authenticate-to-ecr/"/>
        <id>https://devcloud.co.za/how-does-the-docker-client-authenticate-to-ecr/</id>
        <media:content url="https://devcloud.co.za/media/posts/16/ECR-Docker2.png" medium="image" />
            <category term="IAM"/>
            <category term="ECR"/>
            <category term="Docker"/>

        <updated>2022-02-10T18:30:25+02:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://devcloud.co.za/media/posts/16/ECR-Docker2.png" alt="" />
                    If you have been using ECR, or plan to use ECR for your private images, you should be aware that ECR is closely integrated with IAM. You need an IAM user/role with appropriate permissions to access an ECR repository, and you can pull, push, and&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://devcloud.co.za/media/posts/16/ECR-Docker2.png" class="type:primaryImage" alt="" /></p>
                
  <p>
    If you have been using ECR, or plan to use ECR for your private images, you should be aware that ECR is closely integrated with IAM. You need an IAM user/role with appropriate permissions to access an ECR repository, and you can pull, push, and authenticate to the ECR repository using AWS API calls. A basic IAM policy with a list of actions needed to interact with a policy is detailed as below:
  </p>
<pre class="line-numbers  language-json"><code>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ecr:GetAuthorizationToken",
                "ecr:BatchCheckLayerAvailability",
                "ecr:GetDownloadUrlForLayer",
                "ecr:GetRepositoryPolicy",
                "ecr:DescribeRepositories",
                "ecr:ListImages",
                "ecr:DescribeImages",
                "ecr:BatchGetImage",
                "ecr:GetLifecyclePolicy",
                "ecr:GetLifecyclePolicyPreview",
                "ecr:ListTagsForResource",
                "ecr:DescribeImageScanFindings",
                "ecr:InitiateLayerUpload",
                "ecr:UploadLayerPart",
                "ecr:CompleteLayerUpload",
                "ecr:PutImage"
            ],
            "Resource": "*"
        }
    ]
}</code></pre>

  <p>
    The <a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/security-iam-awsmanpol.html" target="_blank" class="" data-link-popup-id="1644497448533">AWS ECR documentation</a> provides several example policies which you can attach to your IAM user/role which will be accessing ECR.
  </p>

  <p>
    You can also place a policy on the repository itself, which is used to limit users and accounts which can access the repository (this is somewhat similar to the effect of an S3 bucket policy). IAM is an excellent source of Authentication (confirms the identity of the user/role making the call) and Authorization (is this identity allowed to make this API call), which are both necessary when securing your image. However, this indicates that an IAM user/role is needed in order to run these API calls. Additionally, the average user would generally interact with these images using a docker client, not using the APIs directly. This begs the question, how exactly does the docker client access an ECR repository is IAM credentials are needed? To investigate this behaviour, lets first analyse how an API call actually works, and how the Docker client this functionality.
  </p>

    <h2 id="signature-version-4">
      Signature Version 4
    </h2>

  <p>
    Signature Version 4 (or SigV4) is the process by which authentication information is added to an API request. We will not go into the process in exact detail, but the general overview is that the HTTP API request includes the API call you would like to make, as well as the AWS Access Key and a signature which is calculated using a hashing algorithm, current date and time, and the Secret Key. The request also needs to include headers for the current date and time (<code>X-Amz-Date</code>), the hashing algorithm (<code>AWS4-HMAC-SHA256</code>), and the session token if you are using an IAM role. The <a href="https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html" target="_blank">AWS documentation for SigV4</a> explains this process in more detail.
  </p>

  <p>
    Coming back to Docker using IAM authentication, there are two issues which we can identify:
  </p>

  <ol>
    <li>Docker would need to be able to translate the Docker push and pull commands into the appropriate ECR API calls. This means Docker needs to be able to obtain the AWS credentials and needs to be built with the AWS eco system in mind.</li><li>Docker has support for a bearer authentication token (explained below) which can contain an <code>Authorization</code> header, but would not be able to contain the&nbsp;<code>X-Amz-Date</code>&nbsp;and&nbsp;<code>AWS4-HMAC-SHA256</code> headers.</li>
  </ol>

  <p>
    In order to address these issues, we leverage the use of the bearer token, and allow the ECR endpoint to handle some of the work.
  </p>

    <h2 id="the-aws-getloginpassword-api-and-docker-authorization-token">
      The AWS GetLoginPassword API and Docker Authorization Token
    </h2>

  <p>
    In order to log into your ECR endpoint, the ECR console provides the below command which you need to run:
  </p>
<pre class="line-numbers  language-bash"><code>aws ecr get-login-password --region &lt;REGION&gt; | docker login --username AWS --password-stdin &lt;ACCOUNT_ID&gt;.dkr.ecr.&lt;REGION&gt;.amazonaws.com</code></pre>

  <p>
    As seen in the command, the AWS CLI is used to obtain the login password used to login to the repository. This allows the AWS CLI to handle obtaining the credentials, and the login password provided would include these credentials. The docker login command would then use this password and return an authorization token which includes the credentials, as well as the required headers which are needed for the API calls. The token is then saved to the <code>~/.docker/config.json</code> file. The contents of this file after logging in would be similar to below:
  </p>
<pre class="line-numbers  language-bash"><code>$ cat ~/.docker/config.json
{
    "auths": {
        "&lt;ACCOUNT_ID&gt;.dkr.ecr.&lt;REGION&gt;.amazonaws.com": {
            "auth": "xxxxxx...."
        }
    }
}
</code></pre>

  <p>
    The Docker client would then use this authorization token whenever it interacts with the ECR repository. The authorization token would contain the credentials of the IAM user/role which was used to login to the repository.&nbsp; When the docker client invokes any action against the repository with this token, it is able to determine access rights of the IAM user/role. In this way, Docker is able to interact with the repository with the security advantages of IAM.
  </p>

  <p>
    The authentication token is also encrypted. It is returned encrypted from ECR when running the docker login command, hence you would not be able to unencrypt this token. However, if you are able to obtain this token, you can run docker commands using that token. This is why the legacy <code>get-login</code> command is no longer recommended. The <a href="https://docs.aws.amazon.com/cli/latest/reference/ecr/get-login.html">AWS CLI documentation for get-login</a> mentions the following:
  </p>
<pre class="line-numbers  language-markdown"><code>Note:

This command displays docker login commands to stdout with authentication credentials. Your credentials could be visible by other users on your system in a process list display or a command history. If you are not on a secure system, you should consider this risk and login interactively. For more information, see get-authorization-token.
</code></pre>

  <p>
    However, as the token is based on your IAM user/role credentials, if you change the IAM user/role used by your machine to access ECR, you need to login to the docker registry again so that a new token can be created using the updated IAM credentials. Additionally, the token is temporary, and is only valid for 12 hours. Once this time period has elapsed, the token would have expired, and you need to login again.
  </p>

    <h2 id="why-use-this-method">
      Why use this method?
    </h2>

  <p>
    The reason behind this approach is that ECR is intended to be easily integrated into your environment, and you can use your existing Docker setup to manage. In this way, you do not need to modify your Docker client in order to take advantage of ECR. The only addition that you would need is the AWS CLI, however you can instead opt to use the <a href="https://github.com/awslabs/amazon-ecr-credential-helper" target="_blank">ECR credential helper</a> instead, which can manage your credentials for you. You should look at utilizing a credential helper in your Docker environment to improve security because, by default, the token is stored in plain text in the&nbsp;<code>~/.docker/config.json</code> file. It also ensures that credentials are periodically refreshed so that the token is always valid. This is useful in setups where you have automated the running of your Docker containers, for example a Jenkins server.
  </p>

    <h2 id="conclusion">
      Conclusion
    </h2>

  <p>
    The main take away from this blog post is to highlight how ECR and IAM work with Docker authorization tokens to allow you to securely access your images by leveraging IAM. Understanding the process helps us to take precautions such as ensuring get-login-password is used instead of get-login, or a credential helper is used to safe guard the token. It can also help troubleshoot any possible issues which may arise, for example, failed image pulls when the IAM role of an instance changed. This can go a long way in furthering your knowledge of setting up a Docker/ECR environment in the AWS cloud, and should make your journey a lot easier.
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>How to store ECS Stopped Task Details</title>
        <author>
            <name>Jenade Moodley</name>
        </author>
        <link href="https://devcloud.co.za/how-to-store-ecs-stopped-task-details/"/>
        <id>https://devcloud.co.za/how-to-store-ecs-stopped-task-details/</id>
        <media:content url="https://devcloud.co.za/media/posts/14/df8a5c58-68a2-4f99-bc4d-3e080890d13a.jpg" medium="image" />
            <category term="ECS"/>
            <category term="AWS"/>

        <updated>2020-10-08T18:21:29+02:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://devcloud.co.za/media/posts/14/df8a5c58-68a2-4f99-bc4d-3e080890d13a.jpg" alt="" />
                    The Importance of Stopped Task Details When you are running containers in ECS, there are times in which the task suddenly stops. Often times these tasks are part of an ECS service and so they will be restarted, but if your architect is not stateless&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://devcloud.co.za/media/posts/14/df8a5c58-68a2-4f99-bc4d-3e080890d13a.jpg" class="type:primaryImage" alt="" /></p>
                
    <h3 id="the-importance-of-stopped-task-details">
      The Importance of Stopped Task Details
    </h3>

  <p>
    When you are running containers in ECS, there are times in which the task suddenly stops. Often times these tasks are part of an ECS service and so they will be restarted, but if your architect is not stateless and those stopped task caused errors, then the task stopping can cause an issue with your application. An example of this would be a container which uses socket connections such as Redis.
  </p>

  <p>
    In such cases, knowing the cause of the task stopping would be of utmost importance as you would need to prevent the task from stopping again. This fact reigns true even more so in production environments, where you also need to report on why any failures occurred in the environment. On ECS, when a task is stopped, you would be able to view the stopped task in your ECS console. When you select your cluster, you can select the Tasks tab which contains a list of stopped tasks.
  </p>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="1">
      <figure class="gallery__item">
      <a href="https://devcloud.co.za/media/posts/14/gallery/Screenshot-8.png" data-size="1444x307">
        <img loading="lazy" src="https://devcloud.co.za/media/posts/14/gallery/Screenshot-8-thumbnail.png" height="153" width="720" alt="Stopped Task" >
      </a>
      <figcaption>Stopped Task</figcaption>
    </figure>
    </div>
  </div>

  <p>
    You can also select the task to view even more task details, as well as the container details (including the exit code) from each container. The container code is extremely useful if the task was stopped due to a container level issue, such as out of memory errors.
  </p>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="3">
      <figure class="gallery__item">
      <a href="https://devcloud.co.za/media/posts/14/gallery/Screenshot-9.png" data-size="1463x755">
        <img loading="lazy" src="https://devcloud.co.za/media/posts/14/gallery/Screenshot-9-thumbnail.png" height="372" width="720" alt="" >
      </a>
      <figcaption>Additional Task Details</figcaption>
    </figure><figure class="gallery__item">
      <a href="https://devcloud.co.za/media/posts/14/gallery/Screenshot-10.png" data-size="1427x789">
        <img loading="lazy" src="https://devcloud.co.za/media/posts/14/gallery/Screenshot-10-thumbnail.png" height="398" width="720" alt="" >
      </a>
      <figcaption>Container Details</figcaption>
    </figure>
    </div>
  </div>

    <h3 id="why-do-we-need-to-store-ecs-stopped-tasks">
      Why do we Need to Store ECS Stopped Tasks?
    </h3>

  <p>
    The above information is certainly useful. Unfortunately, stopped ECS task details only remain in the console for at least an hour, after which it is removed. If you notice that the task was stopped after an hour, chances are that you will not be able to view the reason why the task was stopped. This is mentioned in <a href="https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_ListTasks.html" target="_blank" class="" data-link-popup-id="1602168481251">AWS documentation</a>:<line-separator></line-separator>
  </p>

    <blockquote class="blockquote">
      Recently stopped tasks might appear in the returned results. Currently, stopped tasks appear in the returned results for at least one hour.&nbsp;
    </blockquote>

    <h3 id="implementing-the-solution">
      Implementing the Solution
    </h3>

  <p>
    The solution is actually really simple. When a task is stopped, there is a <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html" target="_blank" class="" data-link-popup-id="1602169359179">CloudWatch event</a> which is triggered to signal that the state of a task has been changed. In ECS, this would be called a <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_cwe_events.html#ecs_task_events" target="_blank" class="" data-link-popup-id="1602169359179">Task state change event</a>. We can then use a service called <a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/what-is-amazon-eventbridge.html" target="_blank" class="" data-link-popup-id="1602169359179">Amazon EventBridge</a>&nbsp;to react to this event, and store the details of the event in a <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html" target="_blank">CloudWatch log stream</a>.&nbsp;
  </p>

  <ol>
    <li>To get started, first navigate to the <a href="https://console.aws.amazon.com/events/" target="_blank">Amazon EventBridge console</a>. Ensure that you are in the same region as the ECS cluster that you want to monitor.</li><li>In the navigation pane, choose <code>Rules</code>.</li><li>Choose <code>Create rule</code>.</li><li>Enter a name and description for the rule. A rule can't have the same name as another rule in the same Region and on the same event bus.</li><li>For <code>Define pattern</code>, choose <code>Event pattern</code>.</li><li>Choose <code>Custom pattern</code>.</li><li>Paste the below JSON snippet into the Event pattern text window and click <code>Save</code>.</li>
  </ol>
<pre class="line-numbers  language-json"><code>{
  "detail-type": [
    "ECS Task State Change"
  ],
  "source": [
    "aws.ecs"
  ],
  "detail": {
    "desiredStatus": [
      "STOPPED"
    ],
    "lastStatus": [
      "STOPPED"
    ]
  }
}</code></pre>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="3">
      <figure class="gallery__item">
      <a href="https://devcloud.co.za/media/posts/14/gallery/Screenshot-11.png" data-size="823x715">
        <img loading="lazy" src="https://devcloud.co.za/media/posts/14/gallery/Screenshot-11-thumbnail.png" height="626" width="720" alt="" >
      </a>
      
    </figure>
    </div>
  </div>
<div><ol start="8">
<li><span style="color: var(--eb-text-primary-color); font-family: var(--font-base); font-size: 1em; font-weight: var(--font-weight-normal);">For <code>Select targets</code>, choose <code>CloudWatch log group</code>.</span><br></li>
<li><span style="color: var(--eb-text-primary-color); font-family: var(--font-base); font-size: 1em; font-weight: var(--font-weight-normal);">Provide a name for the log group, or choose an existing log group.</span></li></ol></div>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="3">
      <figure class="gallery__item">
      <a href="https://devcloud.co.za/media/posts/14/gallery/Screenshot-12.png" data-size="828x493">
        <img loading="lazy" src="https://devcloud.co.za/media/posts/14/gallery/Screenshot-12-thumbnail.png" height="429" width="720" alt="" >
      </a>
      
    </figure>
    </div>
  </div>
<div><ol start="10">
<li>Click <code>Create</code>.</li></ol></div>

    <h3 id="results">
      Results
    </h3>

  <p>
    Should a task be stopped for any reason, the details of the task will be saved to the log stream. The log will be saved under the task ID for the stopped task.
  </p>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="3">
      <figure class="gallery__item">
      <a href="https://devcloud.co.za/media/posts/14/gallery/Screenshot-13-alt.png" data-size="1470x639">
        <img loading="lazy" src="https://devcloud.co.za/media/posts/14/gallery/Screenshot-13-alt-thumbnail.png" height="313" width="720" alt="" >
      </a>
      
    </figure>
    </div>
  </div>

  <p>
    The resulting log entry will contain a JSON output containing the stopped task details.
  </p>
<pre class="line-numbers  language-json"><code>{
    "version": "0",
    "id": "ba5eb4e7-60be-82a2-621a-4a1b11112b5b",
    "detail-type": "ECS Task State Change",
    "source": "aws.ecs",
    "account": "&lt;ACCOUNT_ID_REDACTED&gt;",
    "time": "2020-10-08T14:27:07Z",
    "region": "eu-west-1",
    "resources": [
        "arn:aws:ecs:eu-west-1:&lt;ACCOUNT_ID_REDACTED&gt;:task/default/8993cbec849642b8a9e00da5a620c3fe"
    ],
    "detail": {
        "attachments": [
            {
                "id": "a6fcab45-6aa4-4c37-bd0e-e1a2ad30a6d2",
                "type": "eni",
                "status": "DELETED",
                "details": [
                    {
                        "name": "subnetId",
                        "value": "subnet-b2bea0fb"
                    },
                    {
                        "name": "networkInterfaceId",
                        "value": "eni-05f85d6424640254e"
                    },
                    {
                        "name": "macAddress",
                        "value": "06:c5:43:de:e8:bf"
                    },
                    {
                        "name": "privateIPv4Address",
                        "value": "172.31.20.20"
                    }
                ]
            }
        ],
        "availabilityZone": "eu-west-1b",
        "clusterArn": "arn:aws:ecs:eu-west-1:&lt;ACCOUNT_ID_REDACTED&gt;:cluster/default",
        "containers": [
            {
                "containerArn": "arn:aws:ecs:eu-west-1:&lt;ACCOUNT_ID_REDACTED&gt;:container/45d23d4f-2e5c-4781-8cd7-cb521deae872",
                "exitCode": 0,
                "lastStatus": "STOPPED",
                "name": "Nginx",
                "image": "nginx",
                "runtimeId": "a8aece9082bd0901c023c191be013bbd11c129f288647dfa53002661d8285057",
                "taskArn": "arn:aws:ecs:eu-west-1:&lt;ACCOUNT_ID_REDACTED&gt;:task/default/8993cbec849642b8a9e00da5a620c3fe",
                "networkInterfaces": [
                    {
                        "attachmentId": "a6fcab45-6aa4-4c37-bd0e-e1a2ad30a6d2",
                        "privateIpv4Address": "172.31.20.20"
                    }
                ],
                "cpu": "0"
            }
        ],
        "createdAt": "2020-10-08T14:25:20.227Z",
        "launchType": "FARGATE",
        "cpu": "512",
        "memory": "1024",
        "desiredStatus": "STOPPED",
        "group": "service:nginx-service",
        "lastStatus": "STOPPED",
        "overrides": {
            "containerOverrides": [
                {
                    "name": "Nginx"
                }
            ]
        },
        "connectivity": "CONNECTED",
        "connectivityAt": "2020-10-08T14:25:33.981Z",
        "pullStartedAt": "2020-10-08T14:25:39.75Z",
        "startedAt": "2020-10-08T14:25:48.75Z",
        "startedBy": "ecs-svc/0068978569384120015",
        "stoppingAt": "2020-10-08T14:26:43.833Z",
        "stoppedAt": "2020-10-08T14:27:07.323Z",
        "pullStoppedAt": "2020-10-08T14:25:46.75Z",
        "executionStoppedAt": "2020-10-08T14:26:44Z",
        "stoppedReason": "Task stopped by user",
        "stopCode": "UserInitiated",
        "updatedAt": "2020-10-08T14:27:07.323Z",
        "taskArn": "arn:aws:ecs:eu-west-1:&lt;ACCOUNT_ID_REDACTED&gt;:task/default/8993cbec849642b8a9e00da5a620c3fe",
        "taskDefinitionArn": "arn:aws:ecs:eu-west-1:&lt;ACCOUNT_ID_REDACTED&gt;:task-definition/nginx-basic-fargate:1",
        "version": 6,
        "platformVersion": "1.3.0"
    }
}</code></pre>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="3">
      <figure class="gallery__item">
      <a href="https://devcloud.co.za/media/posts/14/gallery/Screenshot-16.png" data-size="1455x824">
        <img loading="lazy" src="https://devcloud.co.za/media/posts/14/gallery/Screenshot-16-thumbnail.png" height="408" width="720" alt="" >
      </a>
      
    </figure>
    </div>
  </div>

  <p>
    And that's about it. Very simple to setup, and very useful in the event when you do need to troubleshoot stopped tasks. Let me know your thoughts or any concerns in the comments below, and feel free to adjust these steps to meet your use-case.
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>ECS - Monitor container-level CPU and memory using Docker stats and CloudWatch metrics</title>
        <author>
            <name>Jenade Moodley</name>
        </author>
        <link href="https://devcloud.co.za/ecs-monitor-container-level-cpu-and-memory-using-docker-stats-and-cloudwatch-metrics/"/>
        <id>https://devcloud.co.za/ecs-monitor-container-level-cpu-and-memory-using-docker-stats-and-cloudwatch-metrics/</id>
        <media:content url="https://devcloud.co.za/media/posts/13/check-the-version-with-fixes-link-in-description-29-638.jpg" medium="image" />
            <category term="ECS"/>
            <category term="Docker"/>
            <category term="AWS"/>

        <updated>2020-10-08T17:59:25+02:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://devcloud.co.za/media/posts/13/check-the-version-with-fixes-link-in-description-29-638.jpg" alt="" />
                    When using ECS, you run containers in the form of an ECS task, where each task can contain one or more container definitions, up to a maximum of 10. If you want to monitor the performance of your tasks, you can monitor the containers at&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://devcloud.co.za/media/posts/13/check-the-version-with-fixes-link-in-description-29-638.jpg" class="type:primaryImage" alt="" /></p>
                
  <p>
    When using ECS, you run containers in the form of an ECS task, where each task can contain one or more container definitions, up to a maximum of 10. If you want to monitor the performance of your tasks, you can monitor the containers at the task level. This does offer good insight into the working of your task, but it doesn't allow you to view the performance of each container. In a delicate environment where you need to monitor each and every aspect of the system, monitoring each container would be extremely beneficial. There is no CloudWatch metric which monitors container-level resource usage, but it is possible to build our own CloudWatch metric using the container-level resource usage statistics provided by the Docker daemon on the EC2 host. These statistics are obtained by the more widely known <code>docker stats</code> command.
  </p>

  <p>
    The <code>docker stats</code> command returns a live data stream for running containers. With this command, you would be able to obtain the CPU, memory, network I/O, as well as disk I/O usage of each container. For this post, I will only be covering CPU and memory usage. You can take a more detailed look at <code>docker stats</code> from <a href="https://docs.docker.com/engine/reference/commandline/stats/" target="_blank">Docker's documentation</a>.
  </p>

  <p>
    To provide a short overview of the steps we will be deploying, we will first create a script which obtains the resource usage statistics using <code>docker stats</code>, and upload these values to CloudWatch metrics. The statistics obtained will need to be easily obtained and parsed, hence the statistics will be obtained as a <code>JSON</code>, and be parsed using <code>jq</code> (more on this below). This script needs to be run at set intervals in order to gain a good understanding of the container resource usage patterns. To do this, we will create a cron job which will run this script at a set frequency.
  </p>

    <h2 id="installing-the-dependencies">
      Installing the dependencies
    </h2>

  <p>
    As we will be sending the metrics to CloudWatch, and parsing through the <code>JSON</code> statistics using <code>jq</code>, we need to install the AWS CLI, as well as the <code>jq</code> binary. As you are using ECS, you would most likely be using the ECS-optimized AMI, which is based on Amazon Linux, or Amazon Linux 2. As such, the commands you would use to install these dependencies would be as below:
  </p>
<pre class="line-numbers  language-bash"><code>sudo yum install python3 python3-pip jq -y
sudo pip3 install -U awscli</code></pre>

    <h2 id="creating-the-script">
      Creating the script
    </h2>

  <p>
    Let's first look at the commands which will be included in the script.
  </p>

  <p>
    We first need to obtain the resource usage statistics using <code>docker stats</code>. By default,<code> docker stats</code> is presented as text, which is not very easy to handle when trying to filter for specific values. As such, we can format the statistics into a <code>JSON</code> using the <code>--format</code> parameter. This will be based on a publicly available example <a href="https://gist.github.com/KyleBanks/8befbf0ae0d0535fa27d33cb67b40bd1" target="_blank">docker-stats-json</a>. The statistics will then be formed into a valid <code>JSON</code> object using <code>jq</code>.
  </p>
<pre class="line-numbers  language-bash"><code>STATS=$(docker stats --no-stream --format "{\"container\": \"{{ .Container }}\",\"name\": \"{{ .Name }}\", \"memory\": { \"raw\": \"{{ .MemUsage }}\", \"percent\": \"{{ .MemPerc }}\"}, \"cpu\": \"{{ .CPUPerc }}\"}" | jq '.' -s -c )</code></pre>

  <p>
    As we will be uploading the data to CloudWatch metrics, CloudWatch is region-specific. As such we also need to include the region in our script. We can obtain the region programmatically from the metadata service.
  </p>
<pre class="line-numbers  language-bash"><code>REGION=$(curl http://169.254.169.254/latest/meta-data/placement/region 2&gt; /dev/null)</code></pre>

  <p>
    We also need a way to identify and structure our metrics in CloudWatch. One good way to manage this is to use the EC2 instance ID as a dimension for the metric, as well as the Container Name, and Container ID. This will allow us to identify the exact container which provided the metric. As such, we first need the EC2 instance ID, which can also be obtained from the instance metadata.
  </p>
<pre class="line-numbers  language-bash"><code>INSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id 2&gt; /dev/null)</code></pre>

  <p>
    Finally, we need to parse through the JSON to obtain the Container Name, Container ID, CPU, and Memory usage of each container. To do this we first need to obtain the total number of containers.
  </p>
<pre class="line-numbers  language-bash"><code>NUM_CONTAINERS=$(echo "$STATS" | jq '. | length')</code></pre>

  <p>
    We then iterate through the JSON to obtain the values we need.
  </p>
<pre class="line-numbers  language-bash"><code>for (( i=0; i&lt;$NUM_CONTAINERS; i++ )) 
do CPU=$(echo "$STATS" | jq -r .[$i].cpu | sed 's/%//')
MEMORY=$(echo "$STATS" | jq -r .[$i].memory.percent | sed 's/%//')
CONTAINER=$(echo $STATS | jq -r .[$i].container)
CONTAINER_NAME=$(echo $STATS | jq -r .[$i].name)</code></pre>

  <p>
    And then upload these values to CloudWatch metrics using the AWS CLI.
  </p>
<pre class="line-numbers  language-bash"><code>/usr/local/bin/aws cloudwatch put-metric-data --metric-name CPU --namespace DockerStats --unit Percent --value $CPU --dimensions InstanceId=$INSTANCE_ID,ContainerId=$CONTAINER,ContainerName=$CONTAINER_NAME --region $REGION
/usr/local/bin/aws cloudwatch put-metric-data --metric-name Memory --namespace DockerStats --unit Percent --value $MEMORY --dimensions InstanceId=$INSTANCE_ID,ContainerId=$CONTAINER,ContainerName=$CONTAINER_NAME --region $REGION
done</code></pre>

  <p>
    Putting it all together, the completed script should look as below.
  </p>
<pre class="line-numbers  language-bash"><code>#!/bin/bash

REGION=$(curl http://169.254.169.254/latest/meta-data/placement/region 2&gt; /dev/null)
INSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id 2&gt; /dev/null)
STATS=$(docker stats --no-stream --format "{\"container\": \"{{ .Container }}\",\"name\": \"{{ .Name }}\", \"memory\": { \"raw\": \"{{ .MemUsage }}\", \"percent\": \"{{ .MemPerc }}\"}, \"cpu\": \"{{ .CPUPerc }}\"}" | jq '.' -s -c )
NUM_CONTAINERS=$(echo "$STATS" | jq '. | length')

for (( i=0; i&lt;$NUM_CONTAINERS; i++ )) 
do CPU=$(echo "$STATS" | jq -r .[$i].cpu | sed 's/%//')
MEMORY=$(echo "$STATS" | jq -r .[$i].memory.percent | sed 's/%//')
CONTAINER=$(echo $STATS | jq -r .[$i].container)
CONTAINER_NAME=$(echo $STATS | jq -r .[$i].name)

/usr/local/bin/aws cloudwatch put-metric-data --metric-name CPU --namespace DockerStats --unit Percent --value $CPU --dimensions InstanceId=$INSTANCE_ID,ContainerId=$CONTAINER,ContainerName=$CONTAINER_NAME --region $REGION
/usr/local/bin/aws cloudwatch put-metric-data --metric-name Memory --namespace DockerStats --unit Percent --value $MEMORY --dimensions InstanceId=$INSTANCE_ID,ContainerId=$CONTAINER,ContainerName=$CONTAINER_NAME --region $REGION

done</code></pre>

    <h2 id="creating-the-cron-job">
      Creating the Cron job
    </h2>

  <p>
    For the sake of simplicity, I have placed the above script at&nbsp;<code>/opt/docker-stats-monitor/monitor.sh</code> in my EC2 instance.
  </p>

  <p>
    We first need to make the above script executable.
  </p>
<pre class="line-numbers  language-bash"><code>chmod +x /opt/docker-stats-monitor/monitor.sh</code></pre>

  <p>
    We then need to create the cron job by creating a file in /etc/cron.d.
  </p>
<pre class="line-numbers  language-bash"><code>echo '* * * * * root bash /opt/docker-stats-monitor/monitor.sh' &gt; /etc/cron.d/docker-stats</code></pre>

  <p>
    The above example creates a cron job which obtains and uploads the resource-usage statistics every minute. You can adjust the frequency by changing the <a href="https://crontab.guru/" target="_blank">cron schedule expression</a>.
  </p>

    <h2 id="the-results">
      The Results
    </h2>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="3">
      <figure class="gallery__item">
      <a href="https://devcloud.co.za/media/posts/13/gallery/Annotation-2020-08-18-184055.jpg" data-size="1454x577">
        <img loading="lazy" src="https://devcloud.co.za/media/posts/13/gallery/Annotation-2020-08-18-184055-thumbnail.jpg" height="286" width="720" alt="" >
      </a>
      <figcaption>Docker stats as CloudWatch Metrics</figcaption>
    </figure>
    </div>
  </div>

    <h2 id="automating-the-procedure">
      Automating the Procedure
    </h2>

  <p>
    When using ECS, you would hardly access the EC2 host, if at all. You would also most likely be incorporating Auto Scaling or Capacity Providers to launch the EC2 instance, hence you cannot SSH into each instance to configure this setup. To make this a plausible solution, you can implement the above steps using a <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html" target="_blank">User Data script</a>.
  </p>
<pre class="line-numbers  language-bash"><code>#!/bin/bash

yum install python3 python3-pip jq -y
pip3 install -U awscli

mkdir -p /opt/docker-stats-monitor

cat &gt; /opt/docker-stats-monitor/monitor.sh &lt;&lt; 'EOF'
#!/bin/bash

REGION=$(curl http://169.254.169.254/latest/meta-data/placement/region 2&gt; /dev/null)

INSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id 2&gt; /dev/null)

STATS=$(docker stats --no-stream --format "{\"container\": \"{{ .Container }}\",\"name\": \"{{ .Name }}\", \"memory\": { \"raw\": \"{{ .MemUsage }}\", \"percent\": \"{{ .MemPerc }}\"}, \"cpu\": \"{{ .CPUPerc }}\"}" | jq '.' -s -c )

NUM_CONTAINERS=$(echo "$STATS" | jq '. | length')

for (( i=0; i&lt;$NUM_CONTAINERS; i++ )) 
do CPU=$(echo "$STATS" | jq -r .[$i].cpu | sed 's/%//')
MEMORY=$(echo "$STATS" | jq -r .[$i].memory.percent | sed 's/%//')
CONTAINER=$(echo $STATS | jq -r .[$i].container)
CONTAINER_NAME=$(echo $STATS | jq -r .[$i].name)

/usr/local/bin/aws cloudwatch put-metric-data --metric-name CPU --namespace DockerStats --unit Percent --value $CPU --dimensions InstanceId=$INSTANCE_ID,ContainerId=$CONTAINER,ContainerName=$CONTAINER_NAME --region $REGION

/usr/local/bin/aws cloudwatch put-metric-data --metric-name Memory --namespace DockerStats --unit Percent --value $MEMORY --dimensions InstanceId=$INSTANCE_ID,ContainerId=$CONTAINER,ContainerName=$CONTAINER_NAME --region $REGION

done
EOF

chmod +x /opt/docker-stats-monitor/monitor.sh

echo '* * * * * root bash /opt/docker-stats-monitor/monitor.sh' &gt; /etc/cron.d/docker-stats</code></pre>

    <h2 id="conclusion">
      Conclusion
    </h2>

  <p>
    This is a feasible solution for monitoring Docker containers, however one big caveat which you would have noticed is that this solution requires running commands on the EC2 host used for running the containers. This means that this solution can only work for tasks using the EC2 Launch Type, and will not work for Fargate tasks.
  </p>

  <p>
    And that's all there is to it. Let me know your thoughts in the comments below, and feel free to adjust these steps to meet your use-case, and share with the rest of the community.
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>About Me</title>
        <author>
            <name>Jenade Moodley</name>
        </author>
        <link href="https://devcloud.co.za/about-me/"/>
        <id>https://devcloud.co.za/about-me/</id>
        <media:content url="https://devcloud.co.za/media/posts/12/wificoffeelaptop-1280x720-2.jpg" medium="image" />

        <updated>2020-08-17T13:14:54+02:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://devcloud.co.za/media/posts/12/wificoffeelaptop-1280x720-2.jpg" alt="" />
                    Hi There! Feel free to continue to my online Resume. Also available in PDF and JSON.
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://devcloud.co.za/media/posts/12/wificoffeelaptop-1280x720-2.jpg" class="type:primaryImage" alt="" /></p>
                
    <h2 id="hi-there">
      Hi There!
    </h2>

  <p>
    Feel free to continue to my online <a href="https://jenademoodley.co.za/resume/" target="_blank">Resume</a>. Also available in <a href="https://jenademoodley.co.za/resume/resume.pdf" target="_blank" class="">PDF</a> and <a href="https://jenademoodley.co.za/resume/resume.json" target="_blank">JSON</a>.
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>The NFS Buffer Cache</title>
        <author>
            <name>Jenade Moodley</name>
        </author>
        <link href="https://devcloud.co.za/the-nfs-buffer-cache/"/>
        <id>https://devcloud.co.za/the-nfs-buffer-cache/</id>
            <category term="NFS"/>
            <category term="Linux"/>

        <updated>2020-08-12T10:43:08+02:00</updated>
            <summary>
                <![CDATA[
                    If you are using an NFS compatible Distributed File System (such as a Samba server or EFS), you may notice a slight delay when writing files to the file system, more specifically, when you create a new file from one of the NFS clients, it&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                
  <p>
    If you are using an NFS compatible Distributed File System (such as a Samba server or EFS), you may notice a slight delay when writing files to the file system, more specifically, when you create a new file from one of the NFS clients, it may take a couple of seconds longer to appear on another client, even though it is already available on the original client.
<br>
  </p>

  <p>
    This behaviour can be explained by the NFS buffer cache. By default, when you are writing data to a file in Linux, it is first written in memory. This is appropriately called a buffer, i.e. it can be described as a smaller and faster storage area, which sits in front of the actual storage device. This is to improve performance as memory is much faster than a hard disk or solid state drive. The same is true for NFS, where writes are first written to the NFS buffer, before being flushed (or synced) to the NFS storage. This improves performance on the client performing the writes as the clients do not have to wait for the data to be synced to the NFS disk on every write, allowing more writes to be processed.
  </p>

  <p>
    However, if your application is heavily dependent on consistency, i.e. it needs to be available on another client as soon as the write is made, this buffer will not be ideal for you. Instead, you can then opt-in to allow writes to be written directly to the server, bypassing the cache. This can be controlled by the <code>sync</code> NFS mount option. By default, the NFS client will use the <code>async</code> mount option unless specified otherwise. <code>async</code> refers to asynchronous, where data is flushed asynchronously to the main server. If you switch to using the <code>sync</code> mount option, data will be flushed to the server immediately, which allows for greater consistency but at a great performance cost. Referring to the <a href="https://linux.die.net/man/5/nfs" target="_blank">NFS man pages</a>:
  </p>
<pre class="line-numbers  language-bash"><code>The sync mount option
    The NFS client treats the sync mount option differently than some other file systems (refer to mount(8) for a description of the generic sync and async mount options). If neither sync nor async is specified (or if the async option is specified), the NFS client delays sending application writes to the server until any of these events occur:

       Memory pressure forces reclamation of system memory resources.

       An application flushes file data explicitly with sync(2), msync(2), or fsync(3).

       An application closes a file with close(2).

       The file is locked/unlocked via fcntl(2).

    In other words, under normal circumstances, data written by an application may not immediately appear on the server that hosts the file.

    If the sync option is specified on a mount point, any system call that writes data to files on that mount point causes that data to be flushed to the server before the system call returns control to user space. This provides greater data cache coherence among clients, but at a significant performance cost.

    Applications can use the O_SYNC open flag to force application writes to individual files to go to the server immediately without the use of the sync mount option.</code></pre>

  <p>
    Like all NFS mount options, the <code>sync</code> mount option is specified when mounting the file system. As previously mentioned, it should be used with caution as it can cause significant performance impact for write operations.
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Understanding Least Outstanding Requests ALB Load Balancing Algorithm</title>
        <author>
            <name>Jenade Moodley</name>
        </author>
        <link href="https://devcloud.co.za/understanding-alb-least-outstanding-request/"/>
        <id>https://devcloud.co.za/understanding-alb-least-outstanding-request/</id>
            <category term="AWS"/>
            <category term="ALB"/>

        <updated>2020-08-12T10:42:47+02:00</updated>
            <summary>
                <![CDATA[
                    If you are using an Application Load Balancer, you may have come across the "Least Outstanding Requests" (or LOR) load balancing algorithm. A load balancing algorithm dictates how requests are sent to targets. With the default round robin configuration, all requests will be split among&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                
  <p>
    If you are using an Application Load Balancer, you may have come across the "Least Outstanding Requests" (or LOR) load balancing algorithm. A load balancing algorithm dictates how requests are sent to targets. With the default round robin configuration, all requests will be split among the targets equally, regardless of the state of the target. This can cause issues if the requests are not equal (i.e. spiky workloads) as this may result in overloading of requests to one of the targets, and under-utilization of other targets. However, the least outstanding requests algorithm aims to solve this by sending requests to targets with the lowest number of outstanding (or existing) connections. Straight from <a href="https://aws.amazon.com/about-aws/whats-new/2019/11/application-load-balancer-now-supports-least-outstanding-requests-algorithm-for-load-balancing-requests/" target="_blank">AWS' blog post</a>:
  </p>

    <blockquote class="blockquote">
      With this algorithm, as the new request comes in, the load balancer will send it to the target with least number of outstanding requests. Targets processing long-standing requests or having lower processing capabilities are not burdened with more requests and the load is evenly spread across targets. This also helps the new targets to effectively take load off of overloaded targets.
    </blockquote>

    <h2 id="what-is-a-least-outstanding-request">
      What is a "Least Outstanding Request"
    </h2>

  <p>
    Simply put, outstanding requests refers to the number of requests that each target is currently processing. In order for this algorithm to work effectively, the ALB needs to keep track of the number of existing requests per target, and update this list whenever a request is completed, or a new request is made. The ALB will route an incoming request to the target with the lowest number of these requests. This also means that a target could be at the bottom of this list for one request, and at the top of the list for the next request, hence the process has a very fluid state.&nbsp;
  </p>

    <h2 id="some-of-my-targets-receive-more-requests-than-others">
      Some of my targets receive more requests than others
    </h2>

  <p>
    Based on the nature of this algorithm, this can be expected. The ALB will route requests to targets with the least number of outstanding requests at that point in time. It will not consider how many requests have already been sent to that target as these requests would have already been fulfilled.
  </p>

  <p>
    Consider a scenario in which there are no requests served to any of your targets, or if there is an equal number of long lasting connections on all targets. If a new request comes in, that request can be served by any of the targets, and an arbitrary target (let's call this target A) is chosen. If the request is completed, and a new request comes in, the scenario is exactly the same, hence target A can be chosen again. It is not uncommon for there to be an imbalance of requests in such scenarios. This should also not cause any issues with resource utilization as each target would handle it's fair share of requests at any given time.
  </p>

  <p>
    From an analysis view point, we tend to look at the number of requests processed by each target over the course of a period of time (such as over an hour, day, week, month, etc), which often causes confusion. When dealing with the least outstanding requests algorithm, we need to take into account that it does not consider the number of requests over a period of time, but the number of requests that each target is processing at that exact moment in time when the request is received.
  </p>

    <h2 id="when-should-i-use-the-least-outstanding-requests-algorithm">
      When should I use the least outstanding requests algorithm
    </h2>

  <p>
    The least outstanding requests algorithm should be used in either or both of the below scenarios:
  </p>

  <ul>
    <li>Not all of the requests are equal (i.e. requests vary in complexity).</li><li>Each of the targets very in processing capability (for example, each EC2 instance used as a target is a different instance type).</li>
  </ul>

  <p>
    If all of your targets are the same, and you expect a similar form and complexity of all requests, you may not see any benefit to using the least outstanding requests algorithm, and you should consider switching back to the round robin load balancing algorithm.&nbsp;
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>ECS - Dynamic Port Mapping</title>
        <author>
            <name>Jenade Moodley</name>
        </author>
        <link href="https://devcloud.co.za/ecs-dynamic-port-mapping/"/>
        <id>https://devcloud.co.za/ecs-dynamic-port-mapping/</id>
        <media:content url="https://devcloud.co.za/media/posts/9/alb-arch.png" medium="image" />
            <category term="ECS"/>
            <category term="AWS"/>
            <category term="ALB"/>

        <updated>2020-08-12T10:42:34+02:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://devcloud.co.za/media/posts/9/alb-arch.png" alt="" />
                    The Port Mapping Problem A popular function of containers is to run a service such as a web or application server to which clients connect to using HTTP, TCP or socket connections. By default, Docker will allocate a port from the host to the container&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://devcloud.co.za/media/posts/9/alb-arch.png" class="type:primaryImage" alt="" /></p>
                
    <h2 id="the-port-mapping-problem">
      The Port Mapping Problem
    </h2>

  <p>
    A popular function of containers is to run a service such as a web or application server to which clients connect to using HTTP, TCP or socket connections. By default, Docker will allocate a port from the host to the container from the ephemeral port range, but you can manually map or expose a specific host port, such as mapping port 80 on the host to port 8080 on the container. This works fine for individual containers and tasks, but if you need to run another container of the same type, you would then need another host as you cannot bind more than one container to a specific host port (i.e. if there is a container or service listening on a specific port such as port 80, no other service or container can use or bind that specific port.
  </p>

  <p>
    Additionally, if your container is lightweight and uses minimal CPU and memory resources, this results in resource wastage as you would need to launch an entirely new instance simply to cater for this port conflict. If you wish to configure reactive scaling (i.e. scale the number of containers due to demand), this is also a blocking factor as this means that you either need additional EC2 instances on standby (increased cost), or spin up a new instance in accordance to demand (slower scale time). You would have to accurately map and plan your port usage on your EC2 hosts such that you can launch your containers without port conflicts, which increases management overhead.
  </p>

    <h2 id="enter-dynamic-port-mapping">
      Enter Dynamic Port Mapping
    </h2>

  <p>
    If you are going to be using multiple containers to host a service, you would also need to configure a suitable and robust solution in order to access each of these containers. When considering HTTP requests, a popular choice in AWS is to use the Application Load Balancer (ALB), often called a Layer 7 load balancer (referring to layer 7 Application layer in the <a href="https://www.cloudflare.com/learning/ddos/glossary/open-systems-interconnection-model-osi/" target="_blank">OSI model</a>, hence the name). An ALB will allow you to effectively distribute (or balance) requests to multiple targets to which it is connected to. While the port on the ALB to which you will issue requests to will always remain the same (port 80 for HTTP requests, and port 443 for HTTPS requests), the back end targets can each be listening on a different port, for example one target can be listening on port 8080, another can be listening on port 8000, etc. Multiple targets can even belong on the same EC2 instance, provided that the ports are different.
  </p>

  <p>
    Using this functionality, we can apply this behaviour to containers. We can run the containers without specifying the port mapping (i.e. allow Docker to map the container port to an ephemeral port on the host), and then configure these containers to be targets for the ALB. This would allow you to run multiple containers of the same type, on the same EC2 instance, effectively reducing cost and management overhead. The ECS service is also capable of determining which host port your container has been bound to, and can configure the target registration for that specific port and container on the ALB, which allows for a seamless configuration.
  </p>

    <h2 id="setting-up-dynamic-port-mapping">
      Setting up Dynamic Port Mapping
    </h2>

  <p>
    Dynamic Port Mapping is relatively easy to configure. From the ECS service side, all you need to do is set the "Host" port in the container mapping to "0". A JSON snippet of the task definition would therefore look like this:
  </p>
<pre class="line-numbers  language-json"><code>"portMappings": [
        {
          "hostPort": 0,
          "protocol": "tcp",
          "containerPort": 80
        }
      ]</code></pre>

  <p>
    However, additional configurations may need to be done on your VPC and container instances in order to allow the connections to successfully reach the container. As the containers will be mapped to the ephemeral ports of the host, you would need to ensure the following criteria is met:
  </p>

  <ol>
    <li>The security groups attached to your container instances or tasks allow for incoming connections on the ephemeral port range from the ALB nodes.</li><li>The NACLs used by the subnet in which your tasks or container instances are located must allow inbound and outbound connections on the ephemeral port range.</li>
  </ol>

  <p>
    The ephemeral port range used by Docker versions 1.6.0 and later will refer to the&nbsp;<code>/proc/sys/net/ipv4/ip_local_port_range</code> kernel parameter on the host. If this kernel parameter is not available, or if you are using an earlier version of Docker, the default ephemeral port range is used, which ranges from ports&nbsp;49153 through 65535. In general,&nbsp;ports below 32768 fall outside of the ephemeral port range. As a general rule of thumb, you can allow incoming connections from ports 32768 through 65535 in order to account for both scenarios and avoid any issues.
  </p>

    <h2 id="considerations">
      Considerations
    </h2>

  <p>
    As you can now have multiple containers running on a single host to which you can connect to using your ALB, a logical consideration is having too many containers on the same host. This can cause resource contention, and can cause containers to be stopped if the configurations are too restrictive. You should actively configure task and container memory and CPU limits so that each container will be given ample resources on the host in order to function effectively.
  </p>
            ]]>
        </content>
    </entry>
</feed>
