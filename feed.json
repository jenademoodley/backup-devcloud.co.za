{
    "version": "https://jsonfeed.org/version/1",
    "title": "DevCloud",
    "description": "",
    "home_page_url": "https://devcloud.co.za",
    "feed_url": "https://devcloud.co.za/feed.json",
    "user_comment": "",
    "author": {
        "name": "Jenade Moodley"
    },
    "items": [
        {
            "id": "https://devcloud.co.za/about-this-website.html",
            "url": "https://devcloud.co.za/about-this-website.html",
            "title": "About this Website",
            "summary": " This website is a personal project which I have built as a way of sharing my experiences with various technologies, and also allows me to document said experiences for self-reference. It is also a way for me to experiment with serverless architecture; the website&hellip;",
            "content_html": "\n  <p>\n    This website is a personal project which I have built as a way of sharing my experiences with various technologies, and also allows me to document said experiences for self-reference. It is also a way for me to experiment with serverless architecture; the website is a static website built using Publii CMS (Content Management System), and is hosted on GitHub pages. DNS and SSL is managed by CloudFlare, and the site makes use of CloudFlare CDN as well to boost performance.\n  </p>\n\n    <h2 id=\"so-you-noticed-there-were-no-ads\">\n      So you noticed there were no ads\n    </h2>\n\n  <p>\n    All of the above components (Publii, GitHub pages, CloudFlare) all have an impressive free tier offering, essentially allowing you to run and manage your own website at virtually no cost. As there are no costs involved with this website, there are no ads included. This allows for a clean and fast user experience.\n  </p>\n\n    <h2 id=\"looking-to-the-future\">\n      Looking to the Future\n    </h2>\n\n  <p>\n    At present, this website is basically a platform for me to share my knowledge and experiences, as well as a playground for testing serverless architecture. Any future plans for this website for this website would depend on the reception and feedback received on the posted content. As the website is still new, no such plans have been made.\n  </p>\n\n  <p>\n    Looking at short-term goals, I will be adding more content in various categories including Linux and containers, and possibly even a coding section. I will also be adding additional AWS based content based on my experiences with certain services (such as EC2, ECS, and EFS).\n  </p>\n\n  <p>\n    And finally, feel free to share these posts if they were a worthwhile read, or if they have helped you with any issues you may have faced.&nbsp;I hope you have fun reading these posts as I have had writing them.&nbsp;\n  </p>",
            "image": "https://devcloud.co.za/media/posts/8/Optimizing-Images-for-The-Web.jpg",
            "author": {
                "name": "Jenade Moodley"
            },
            "tags": [
            ],
            "date_published": "2020-07-20T21:01:20+02:00",
            "date_modified": "2020-07-20T21:01:20+02:00"
        },
        {
            "id": "https://devcloud.co.za/improving-efs-performance-part-2.html",
            "url": "https://devcloud.co.za/improving-efs-performance-part-2.html",
            "title": "Improving EFS Performance (Part 2)",
            "summary": " As discussed in the part 1 of this blog post, the latency which occurs with EFS is per-file operation latency due to the nature of the NFS protocol, specifically with handling metadata. In order to improve performance of your application, you need to understand&hellip;",
            "content_html": "\n  <p>\n    As discussed in the <a href=\"https://devcloud.co.za/improving-efs-performance-part-1.html\">part 1</a> of this blog post, the latency which occurs with EFS is per-file operation latency due to the nature of the NFS protocol, specifically with handling metadata. In order to improve performance of your application, you need to understand the nature of your application, i.e. is your application performing mainly read or write operations. There are a few ways to determine this, which is discussed in the NFS troubleshooting blog post, however an easy way to determine the activity of your EFS on your EC2 instance is to use the <code>mountstats</code> command.\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>mountstats /mnt/efs\nStats for 127.0.0.1:/ mounted on /mnt/efs:\n...\n\nNFS byte counts:\n  applications read 404148 bytes via read(2)\n  applications wrote 1966484148 bytes via write(2)\n  applications read 0 bytes via O_DIRECT read(2)\n  applications wrote 0 bytes via O_DIRECT write(2)\n  client read 0 bytes via NFS READ\n  client wrote 1966484148 bytes via NFS WRITE\n\nRPC statistics:\n  599130 RPC requests sent, 599130 RPC replies received (0 XIDs not found)\n  average backlog queue length: 0\n\nGETATTR:\n        126739 ops (21%)\n        avg bytes sent per op: 313      avg bytes received per op: 240\n        backlog wait: 0.005097  RTT: 1.268323   total execute time: 1.311964 (milliseconds)\nSETATTR:\n        122786 ops (20%)\n        avg bytes sent per op: 360      avg bytes received per op: 260\n        backlog wait: 0.004838  RTT: 4.192335   total execute time: 4.237853 (milliseconds)\nLOOKUP:\n        115717 ops (19%)\n        avg bytes sent per op: 364      avg bytes received per op: 268\n        backlog wait: 0.023964  RTT: 2.415324   total execute time: 2.497403 (milliseconds)\nACCESS:\n        72755 ops (12%)\n        avg bytes sent per op: 320      avg bytes received per op: 168\n        backlog wait: 0.008604  RTT: 1.881768   total execute time: 1.959151 (milliseconds)\nREMOVE:\n        40000 ops (6%)\n        avg bytes sent per op: 346      avg bytes received per op: 116\n        backlog wait: 0.003750  RTT: 7.256800   total execute time: 7.280375 (milliseconds)\nOPEN:\n        30225 ops (5%)\n        avg bytes sent per op: 455      avg bytes received per op: 441\n        backlog wait: 0.016973  RTT: 5.637519   total execute time: 5.663623 (milliseconds)\nCLOSE:\n        30006 ops (5%)\n        avg bytes sent per op: 336      avg bytes received per op: 176\n        backlog wait: 0.010231  RTT: 0.626241   total execute time: 0.643738 (milliseconds)\nWRITE:\n        30002 ops (5%)\n        avg bytes sent per op: 65893    avg bytes received per op: 176\n        backlog wait: 0.012932  RTT: 18.157989  total execute time: 18.181355 (milliseconds)\nRENAME:\n        30000 ops (5%)\n        avg bytes sent per op: 528      avg bytes received per op: 152\n        backlog wait: 0.007867  RTT: 5.331867   total execute time: 5.347367 (milliseconds)\nREADDIR:\n        867 ops (0%)\n        avg bytes sent per op: 342      avg bytes received per op: 32059\n        backlog wait: 0.003460  RTT: 6.455594   total execute time: 6.489043 (milliseconds)\nOPEN_NOATTR:\n        5 ops (0%)\n        avg bytes sent per op: 389      avg bytes received per op: 275\n        backlog wait: 0.000000  RTT: 2.800000   total execute time: 2.800000 (milliseconds)\nSERVER_CAPS:\n        3 ops (0%)\n        avg bytes sent per op: 316      avg bytes received per op: 164\n        backlog wait: 0.000000  RTT: 0.333333   total execute time: 0.333333 (milliseconds)\nFSINFO:\n        2 ops (0%)\n        avg bytes sent per op: 312      avg bytes received per op: 152\n        backlog wait: 0.000000  RTT: 0.500000   total execute time: 0.500000 (milliseconds)\nLOOKUP_ROOT:\n        1 ops (0%)\n        avg bytes sent per op: 184      avg bytes received per op: 380\n        backlog wait: 0.000000  RTT: 0.000000   total execute time: 0.000000 (milliseconds)\nPATHCONF:\n        1 ops (0%)\n        avg bytes sent per op: 308      avg bytes received per op: 116\n        backlog wait: 0.000000  RTT: 0.000000   total execute time: 0.000000 (milliseconds)\nSECINFO_NO_NAME:\n        1 ops (0%)\n        avg bytes sent per op: 172      avg bytes received per op: 104\n        backlog wait: 0.000000  RTT: 5.000000   total execute time: 6.000000 (milliseconds)\n</code></pre>\n\n  <p>\n    As you can see, my EFS is performing write heavy operations, and you can even see the RPC calls made most often to the EFS, the majority of which is <code>GETATTR</code> and <code>SETATTR</code>. These are for setting metadata attributes, indicating that my workload is also metadata based.\n  </p>\n\n  <p>\n    Depending on the type or workload you have, i.e. read intensive or write intensive workloads, there are different options you can configure in order to enhance the performance of your application using EFS.\n  </p>\n\n    <h2 id=\"improving-read-operation-performance\">\n      Improving read operation performance\n    </h2>\n\n  <p>\n    As we discussed, latency experienced on EFS is due to metadata access sequence of the NFS protocol. We cannot change how files are accessed from the EFS, however, we know that the local disk used is faster than EFS. Therefore, if we want to improve access time for files, we can cache the files on the local disk, effectively reducing latency by taking the NFS protocol out of the equation. This can be done using a file system cache (or fs-cache), which was created for this very purpose. A popular file system cache we can use is <code>cachefilesd</code>. To show the effects of using a cache, we can perform a test in which we access a file before and after using the cache to observe the performance improvements.\n  </p>\n\n    <h4 id=\"setting-up-cachefilesd\">\n      Setting up cachefilesd\n    </h4>\n\n  <p>\n    We first need to install <code>cachefilesd</code> using the standard package manager of your distribution.\n  </p>\n\n    <h6 id=\"amazon-linux-centos-rhel\">\n      Amazon Linux, CentOS, RHEL\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>sudo yum install cachefilesd -y</code></pre>\n\n    <h6 id=\"ubuntu\">\n      Ubuntu\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>sudo apt-get update\nsudo apt-get install cachefilesd</code></pre>\n\n  <p>\n    After installing the package, we need to start the <code>cachefilesd</code> daemon.\n  </p>\n\n    <h6 id=\"systemd\">\n      Systemd\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>sudo systemctl start cachefilesd</code></pre>\n\n    <h6 id=\"upstart\">\n      Upstart\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>sudo service cachefilesd start</code></pre>\n\n  <p>\n    We then need to instruct the NFS client to use the cache when accessing the EFS by specifying the <code>fsc</code> mount option when mounting the EFS.\n  </p>\n\n    <h6 id=\"using-the-efs-mount-helper\">\n      Using the EFS Mount Helper\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>sudo mount -t efs -o fsc &lt;EFS_ID&gt; &lt;MOUNT_POINT&gt;</code></pre>\n\n    <h6 id=\"using-the-nfs-client\">\n      Using the NFS client\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>sudo mount -t nfs4 -o fsc,nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport &lt;EFS_DNS&gt;:/ &lt;MOUNT_POINT&gt;</code></pre>\n\n    <h4 id=\"testing-the-cache\">\n      Testing the cache\n    </h4>\n\n  <p>\n    To test the cache, we first need to create a small file on the EFS which we will be accessing.\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>dd if=/dev/urandom of=/mnt/efs/cache_file bs=4k count=128</code></pre>\n\n  <p>\n    To ensure that the cache is not in effect, we can first clear the pagecache, dentries and inodes using&nbsp;<code>/proc/sys/vm/drop_caches</code>\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>sudo echo 3 | sudo tee /proc/sys/vm/drop_caches && sudo sync</code></pre>\n\n  <p>\n    We can then test an uncached access to the file we initially created.\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>time dd if=/mnt/efs/cache_file of=$HOME/local_file bs=4k count=128\n128+0 records in\n128+0 records out\n524288 bytes (524 kB) copied, 0.0176215 s, 29.8 MB/s\n\nreal    0m0.027s\nuser    0m0.002s\nsys     0m0.000s</code></pre>\n\n  <p>\n    First time accessing the file from the EFS took 0.027 seconds, at a speed of 29.8 MB/s. If we access this file again, this file will be served from the cache.\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>time dd if=/mnt/efs/cache_file of=$HOME/local_file bs=4k count=128\n128+0 records in\n128+0 records out\n524288 bytes (524 kB) copied, 0.00155109 s, 338 MB/s\n\nreal    0m0.009s\nuser    0m0.002s\nsys     0m0.000s</code></pre>\n\n  <p>\n    The time taken to access the file was 0.009 seconds at a speed of 338 MB/s. This is 3 times faster than the initial access time. While the access time may seem small, as the latency occurs per-file operation, this time will drastically increase for larger number of files in deeper directories.\n  </p>\n\n    <h4 id=\"limitations-and-caveats\">\n      Limitations and Caveats\n    </h4>\n\n  <p>\n    By definition, the operating system will check the cache first before accessing the EFS. This means that, should a file not exist, the entire cache will be checked before accessing the EFS. This can induce performance issues if the cache is very large and new files are being accessed more frequently than older files. This can be mitigated by configuring more options on cachefilesd such as cache culling, which&nbsp;involves discarding objects from the cache that have been used less recently than other objects. This is explained further in the <code>cachefilesd</code> <a href=\"https://linux.die.net/man/5/cachefilesd.conf\" target=\"_blank\">man pages</a>.\n  </p>\n\n    <h2 id=\"improving-write-operation-performance\">\n      Improving write operation performance\n    </h2>\n\n  <p>\n    While using <code>cachefilesd</code> can speed up access to an existing file, it does not help much for write operations as the file may not yet exist (in the case of new files), and the NFS protocol would still need to access the metadata in order to perform updates such as updating the last modified time, and update the block size. In order to get around this limitation, we can take advantage of the resilient nature of EFS.&nbsp;\n  </p>\n\n  <p>\n    EFS is designed to be accessed by thousands of clients simultaneously, and emphasis is placed on throughput of data written to the file system from all clients. This means that EFS is more situated for parallel workloads, or rather it is with parallel workloads that EFS really shines. As such, we can improve write performance by performing our commands in parallel. This can be done using two methods:\n  </p>\n\n  <ol>\n    <li>Use multiple clients (i.e. use multiple EC2 instances).&nbsp;</li><li>Use a tool such as GNU parallel or <code>fpsync</code> to run the commands in parallel.</li>\n  </ol>\n\n  <p>\n    The first option is the more recommended option as it may be difficult to rebuild your application to execute operations in parallel. However, for regular commands run on multiple files such as <code>cp</code>, <code>mv</code>, or <code>rsync</code> would benefit greatly if configured to operate on files in parallel. By default, these commands perform serial operations, i.e. it would perform the action on one file at a time. Performing the operations on multiple files simultaneously with a single command would greatly improve the performance. To demonstrate the results, as well as the ease of using a parallel utility, we can perform the below test using GNU parallel.&nbsp;<br>\n  </p>\n\n    <h4 id=\"installing-gnu-parallel\">\n      Installing GNU Parallel\n    </h4>\n\n  <p>\n    The commands to install GNU parallel would differ per distribution. For RPM-based distributions (RedHat, CentOS, Amazon Linux), it is available in the <code>epel</code> repository. For Ubuntu, it is available in the default repository. The commands to install GNU parallel would be as below:\n  </p>\n\n    <h6 id=\"rhel-7\">\n      RHEL 7\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\nsudo yum install parallel -y</code></pre>\n\n    <h6 id=\"amazon-linux-rhel-6\">\n      Amazon Linux, RHEL 6\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm\nsudo yum install parallel -y</code></pre>\n\n    <h6 id=\"amazon-linux-2\">\n      Amazon Linux 2\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>sudo amazon-linux-extras install epel\nsudo yum install parallel -y\n</code></pre>\n\n    <h6 id=\"centos-6-centos-7\">\n      CentOS 6, CentOS 7\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>sudo yum install epel-release\nsudo yum install parallel -y\n</code></pre>\n\n    <h6 id=\"ubuntu\">\n      Ubuntu\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>sudo apt-get update\nsudo apt-get install parallel</code></pre>\n\n    <h4 id=\"running-commands-in-parallel\">\n      Running commands in parallel\n    </h4>\n\n  <p>\n    We first need to create a large number of small files on our local disk. In my test, I have created 10000 files. For reference, I am using a standard <code>t2.large</code>&nbsp;EC2 instance running Amazon Linux 2, with a general purpose (gp2) volume of 8 GB in size.\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>mkdir /tmp/efs; for each in $(seq 1 10000); do SUFFIX=$(mktemp -u _XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX); sudo dd if=/dev/zero of=/tmp/efs/${SUFFIX} bs=64k count=1; done</code></pre>\n\n  <p>\n    We can then copy these files to the EFS with <code>rsync</code> using the standard serial method, and measure the time it takes to copy the files.\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>cd /tmp/efs; time sudo find -L . -maxdepth 1 -type f -exec rsync -avR '{}' /mnt/efs/ \\;\n...\nreal    9m11.494s\nuser    0m34.186s\nsys     0m10.303s</code></pre>\n\n  <p>\n    The results show that it took longer than 9 minutes to copy 10000 files to the EFS using <code>rsync</code>. If we purge the directory, and use <code>rsync</code> in parallel:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>cd /tmp/efs; time find -L . -maxdepth 1 -type f | sudo parallel rsync -avR {} /mnt/efs/\n...\nreal    5m41.274s\nuser    1m1.279s\nsys     0m37.556s\n</code></pre>\n\n  <p>\n    The files took longer than 5 minutes, effectively reducing the copy time by almost half. Furthermore, GNU parallel runs as many parallel jobs as there are CPU cores. As the <code>t2.large</code> instance contains 2 cores, this explains why the time was almost halved. However, GNU parallel allows you to specify the number of jobs by using the<code> --jobs</code> flag. If we set the <code>--jobs</code> flag to 0, it would try to run as many jobs as possible with the amount of CPU available.\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>cd /tmp/efs; time find -L . -maxdepth 1 -type f | sudo parallel --jobs 0 rsync -avR {} /mnt/efs/\n...\nreal    3m25.578s\nuser    1m2.320s\nsys     0m45.163s\n</code></pre>\n\n  <p>\n    Using as many jobs as possible, we reduced the time taken to about a third of the time it would have taken when using the serial copy method. You can also edit and configure GNU parallel commands as your needs. You can find a more detailed tutorial of GNU parallel <a href=\"https://www.gnu.org/software/parallel/parallel_tutorial.html#GNU-Parallel-Tutorial\">here</a>.\n  </p>\n\n    <h2 id=\"conclusion\">\n      Conclusion\n    </h2>\n\n  <p>\n    Knowing your application's usage patterns can greatly assist you with configuring your EFS. Using the above suggestions correctly can help you to use the EFS to it's full potential, however incorrect use of these suggestions, such as implementing <code>cachefilesd</code> for an application which only accesses recent data on the EFS, can cause further performance degradation. There are additional caching options you can look at such as <a href=\"https://www.php.net/manual/en/book.opcache.php\" target=\"_blank\">PHP-opcache</a> for PHP-based websites, and incorporating a CDN such as <a href=\"https://aws.amazon.com/cloudfront/\" target=\"_blank\">CloudFront</a>, however these are website specific caching mechanisms. You need to identify a good mix of services and caching mechanisms in order to improve your application performance on EFS, and I hope this blog post gives you a good starting point on improving your experience with EFS.\n  </p>",
            "image": "https://devcloud.co.za/media/posts/7/AWS_EFS-system_DIMq8h4.jpg",
            "author": {
                "name": "Jenade Moodley"
            },
            "tags": [
            ],
            "date_published": "2020-07-20T21:01:20+02:00",
            "date_modified": "2020-07-20T21:01:20+02:00"
        },
        {
            "id": "https://devcloud.co.za/improving-efs-performance-part-1.html",
            "url": "https://devcloud.co.za/improving-efs-performance-part-1.html",
            "title": "Improving EFS Performance (Part 1)",
            "summary": " Why is my EFS slow? If you have started using EFS as the main root storage for your application such as your WordPress website, or a common storage layer for your containerized applications, you may notice that the performance obtained when using the EFS&hellip;",
            "content_html": "\n    <h2 id=\"why-is-my-efs-slow\">\n      Why is my EFS slow?\n    </h2>\n\n  <p>\n    If you have started using EFS as the main root storage for your application such as your WordPress website, or a common storage layer for your containerized applications, you may notice that the performance obtained when using the EFS appears to be slower than using regular disks. This slowness is not caused deliberately, rather it is due to the nature of EFS.&nbsp;\n  </p>\n\n  <p>\n    Using EFS without being aware of it's architecture and limitations can catch you unawares, however, by understanding how EFS works and the nature of your application, it is possible to boost the performance of your application using EFS (notice I said application performance and not EFS performance.)\n  </p>\n\n    <h2 id=\"efs-architecture\">\n      EFS Architecture\n    </h2>\n\n  <p>\n    Straight from AWS' documentation, EFS is a distributed file system which is designed to be accessed by thousands of clients simultaneously, provide redundant and highly available storage across multiple availability zones, and provide low latency per-file operation. The latter of these claims otherwise towards the slowness experienced, however the key-words to remember is \"per-file operation\". It is an NFS file system, and is designed to be accessed via the NFS protocol. Supported versions of the protocol are NFS version 4, and NFS version 4.1 (preferred).\n  </p>\n\n  <p>\n    EFS manages all file storage infrastructure for you, and you only pay for the storage that you use. You do not have to provision any servers or instances, nor do you have to manage any storage or disks. It also guarantees \"close-to-open\" data consistency and supports POSIX permissions.\n  </p>\n\n    <h2 id=\"understanding-perfile-operation-latencynbsp\">\n      Understanding \"per-file operation\" latency&nbsp;\n    </h2>\n\n  <p>\n    To understand why per-file operation latency can induce slowness, we first need to understand how the operating system works with files in general. EFS, at present, can only be accessed by Linux-based clients, and every single component of Linux, be it an application or configuration, consists of files. In short, every action on your Linux operating system requires accessing a file.\n  </p>\n\n  <p>\n    Unpacking that analogy, there are only two options which are applicable to the use of files; reading, and writing to a file. From the operating system's point of view, either of these actions is interpreted as an \"operation\", and there are appropriate read and write operations available as the operating system would need to perform different actions for these operations. This is unpacked further here (coming soon).\n  </p>\n\n  <p>\n    When reading or writing to a file, the operating system needs to first obtain the metadata for the file, which contains various information about the file including the inode of the file, the permissions to determine if the file is accessible by the user or application performing the operation, as well as the block size and IO block (location on the disk) for the actual data on the file. If the user or application has sufficient permissions to access the file, the operating system would then access the contents of the file from the disk.\n  </p>\n\n  <p>\n    Coming back to EFS, we know that EFS uses the NFS protocol to run file operations. What you may not know, is that NFS is not too efficient with handling metadata, which is necessary for all file operations. To briefly explain, your file would be located within a directory, such as the root EFS directory or several directories within the EFS. When trying to access a file, the NFS protocol will recursively check each directory to determine if the user has sufficient permissions to access each directory, before finally arriving at the specified file. As you are connecting to the EFS over a network, each of these checks is a Remote Procedure Call (RPC). As these checks occur over a network, while these checks occur very fast, they are much slower when compared to directly attached storage options. A deeper overview of the NFS protocol sequence is explained <a href=\"http://www.eventhelix.com/realtimemantra/networking/NFS_Protocol_Sequence_Diagram.pdf\" target=\"_blank\">here</a>.\n  </p>\n\n    <h2 id=\"whats-next\">\n      What's next?\n    </h2>\n\n  <p>\n    Now that we understand what causes latency with the EFS, we can go ahead and start looking methods for improving the performance of the applications which use the EFS. This is discussed in the <a href=\"https://devcloud.co.za/improving-efs-performance-part-2.html\">second part of this blog post</a>.\n  </p>",
            "image": "https://devcloud.co.za/media/posts/6/AWS_EFS-system_DIMq8h4.jpg",
            "author": {
                "name": "Jenade Moodley"
            },
            "tags": [
            ],
            "date_published": "2020-07-20T21:01:20+02:00",
            "date_modified": "2020-07-20T21:01:20+02:00"
        }
    ]
}
