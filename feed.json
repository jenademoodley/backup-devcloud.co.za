{
    "version": "https://jsonfeed.org/version/1",
    "title": "DevCloud",
    "description": "",
    "home_page_url": "https://devcloud.co.za",
    "feed_url": "https://devcloud.co.za/feed.json",
    "user_comment": "",
    "author": {
        "name": "Jenade Moodley"
    },
    "items": [
        {
            "id": "https://devcloud.co.za/the-nfs-buffer-cache/",
            "url": "https://devcloud.co.za/the-nfs-buffer-cache/",
            "title": "The NFS Buffer Cache",
            "summary": " If you are using an NFS compatible Distributed File System (such as a Samba server or EFS), you may notice a slight delay when writing files to the file system, more specifically, when you create a new file from one of the NFS clients,&hellip;",
            "content_html": "\n  <p>\n    If you are using an NFS compatible Distributed File System (such as a Samba server or EFS), you may notice a slight delay when writing files to the file system, more specifically, when you create a new file from one of the NFS clients, it may take a couple of seconds longer to appear on another client, even though it is already available on the original client.\n<br>\n  </p>\n\n  <p>\n    This behaviour can be explained by the NFS buffer cache. By default, when you are writing data to a file in Linux, it is first written in memory. This is appropriately called a buffer, i.e. it can be described as a smaller and faster storage area, which sits in front of the actual storage device. This is to improve performance as memory is much faster than a hard disk or solid state drive. The same is true for NFS, where writes are first written to the NFS buffer, before being flushed (or synced) to the NFS storage. This improves performance on the client performing the writes as the clients do not have to wait for the data to be synced to the NFS disk on every write, allowing more writes to be processed.\n  </p>\n\n  <p>\n    However, if your application is heavily dependent on consistency, i.e. it needs to be available on another client as soon as the write is made, this buffer will not be ideal for you. Instead, you can then opt-in to allow writes to be written directly to the server, bypassing the cache. This can be controlled by the <code>sync</code> NFS mount option. By default, the NFS client will use the <code>async</code> mount option unless specified otherwise. <code>async</code> refers to asynchronous, where data is flushed asynchronously to the main server. If you switch to using the <code>sync</code> mount option, data will be flushed to the server immediately, which allows for greater consistency but at a great performance cost. Referring to the <a href=\"https://linux.die.net/man/5/nfs\" target=\"_blank\">NFS man pages</a>:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>The sync mount option\n    The NFS client treats the sync mount option differently than some other file systems (refer to mount(8) for a description of the generic sync and async mount options). If neither sync nor async is specified (or if the async option is specified), the NFS client delays sending application writes to the server until any of these events occur:\n\n       Memory pressure forces reclamation of system memory resources.\n\n       An application flushes file data explicitly with sync(2), msync(2), or fsync(3).\n\n       An application closes a file with close(2).\n\n       The file is locked/unlocked via fcntl(2).\n\n    In other words, under normal circumstances, data written by an application may not immediately appear on the server that hosts the file.\n\n    If the sync option is specified on a mount point, any system call that writes data to files on that mount point causes that data to be flushed to the server before the system call returns control to user space. This provides greater data cache coherence among clients, but at a significant performance cost.\n\n    Applications can use the O_SYNC open flag to force application writes to individual files to go to the server immediately without the use of the sync mount option.</code></pre>\n\n  <p>\n    Like all NFS mount options, the <code>sync</code> mount option is specified when mounting the file system. As previously mentioned, it should be used with caution as it can cause significant performance impact for write operations.\n  </p>",
            "author": {
                "name": "Jenade Moodley"
            },
            "tags": [
                   "NFS",
                   "Linux"
            ],
            "date_published": "2020-08-12T10:43:35+02:00",
            "date_modified": "2020-08-12T10:43:35+02:00"
        },
        {
            "id": "https://devcloud.co.za/understanding-alb-least-outstanding-request/",
            "url": "https://devcloud.co.za/understanding-alb-least-outstanding-request/",
            "title": "Understanding Least Outstanding Requests ALB Load Balancing Algorithm",
            "summary": " If you are using an Application Load Balancer, you may have come across the \"Least Outstanding Requests\" (or LOR) load balancing algorithm. A load balancing algorithm dictates how requests are sent to targets. With the default round robin configuration, all requests will be split&hellip;",
            "content_html": "\n  <p>\n    If you are using an Application Load Balancer, you may have come across the \"Least Outstanding Requests\" (or LOR) load balancing algorithm. A load balancing algorithm dictates how requests are sent to targets. With the default round robin configuration, all requests will be split among the targets equally, regardless of the state of the target. This can cause issues if the requests are not equal (i.e. spiky workloads) as this may result in overloading of requests to one of the targets, and under-utilization of other targets. However, the least outstanding requests algorithm aims to solve this by sending requests to targets with the lowest number of outstanding (or existing) connections. Straight from <a href=\"https://aws.amazon.com/about-aws/whats-new/2019/11/application-load-balancer-now-supports-least-outstanding-requests-algorithm-for-load-balancing-requests/\" target=\"_blank\">AWS' blog post</a>:\n  </p>\n\n    <blockquote class=\"blockquote\">\n      With this algorithm, as the new request comes in, the load balancer will send it to the target with least number of outstanding requests. Targets processing long-standing requests or having lower processing capabilities are not burdened with more requests and the load is evenly spread across targets. This also helps the new targets to effectively take load off of overloaded targets.\n    </blockquote>\n\n    <h2 id=\"what-is-a-least-outstanding-request\">\n      What is a \"Least Outstanding Request\"\n    </h2>\n\n  <p>\n    Simply put, outstanding requests refers to the number of requests that each target is currently processing. In order for this algorithm to work effectively, the ALB needs to keep track of the number of existing requests per target, and update this list whenever a request is completed, or a new request is made. The ALB will route an incoming request to the target with the lowest number of these requests. This also means that a target could be at the bottom of this list for one request, and at the top of the list for the next request, hence the process has a very fluid state.&nbsp;\n  </p>\n\n    <h2 id=\"some-of-my-targets-receive-more-requests-than-others\">\n      Some of my targets receive more requests than others\n    </h2>\n\n  <p>\n    Based on the nature of this algorithm, this can be expected. The ALB will route requests to targets with the least number of outstanding requests at that point in time. It will not consider how many requests have already been sent to that target as these requests would have already been fulfilled.\n  </p>\n\n  <p>\n    Consider a scenario in which there are no requests served to any of your targets, or if there is an equal number of long lasting connections on all targets. If a new request comes in, that request can be served by any of the targets, and an arbitrary target (let's call this target A) is chosen. If the request is completed, and a new request comes in, the scenario is exactly the same, hence target A can be chosen again. It is not uncommon for there to be an imbalance of requests in such scenarios. This should also not cause any issues with resource utilization as each target would handle it's fair share of requests at any given time.\n  </p>\n\n  <p>\n    From an analysis view point, we tend to look at the number of requests processed by each target over the course of a period of time (such as over an hour, day, week, month, etc), which often causes confusion. When dealing with the least outstanding requests algorithm, we need to take into account that it does not consider the number of requests over a period of time, but the number of requests that each target is processing at that exact moment in time when the request is received.\n  </p>\n\n    <h2 id=\"when-should-i-use-the-least-outstanding-requests-algorithm\">\n      When should I use the least outstanding requests algorithm\n    </h2>\n\n  <p>\n    The least outstanding requests algorithm should be used in either or both of the below scenarios:\n  </p>\n\n  <ul>\n    <li>Not all of the requests are equal (i.e. requests vary in complexity).</li><li>Each of the targets very in processing capability (for example, each EC2 instance used as a target is a different instance type).</li>\n  </ul>\n\n  <p>\n    If all of your targets are the same, and you expect a similar form and complexity of all requests, you may not see any benefit to using the least outstanding requests algorithm, and you should consider switching back to the round robin load balancing algorithm.&nbsp;\n  </p>",
            "author": {
                "name": "Jenade Moodley"
            },
            "tags": [
                   "AWS",
                   "ALB"
            ],
            "date_published": "2020-08-12T10:43:35+02:00",
            "date_modified": "2020-08-12T10:43:35+02:00"
        },
        {
            "id": "https://devcloud.co.za/ecs-dynamic-port-mapping/",
            "url": "https://devcloud.co.za/ecs-dynamic-port-mapping/",
            "title": "ECS - Dynamic Port Mapping",
            "summary": " The Port Mapping Problem A popular function of containers is to run a service such as a web or application server to which clients connect to using HTTP, TCP or socket connections. By default, Docker will allocate a port from the host to the&hellip;",
            "content_html": "\n    <h2 id=\"the-port-mapping-problem\">\n      The Port Mapping Problem\n    </h2>\n\n  <p>\n    A popular function of containers is to run a service such as a web or application server to which clients connect to using HTTP, TCP or socket connections. By default, Docker will allocate a port from the host to the container from the ephemeral port range, but you can manually map or expose a specific host port, such as mapping port 80 on the host to port 8080 on the container. This works fine for individual containers and tasks, but if you need to run another container of the same type, you would then need another host as you cannot bind more than one container to a specific host port (i.e. if there is a container or service listening on a specific port such as port 80, no other service or container can use or bind that specific port.\n  </p>\n\n  <p>\n    Additionally, if your container is lightweight and uses minimal CPU and memory resources, this results in resource wastage as you would need to launch an entirely new instance simply to cater for this port conflict. If you wish to configure reactive scaling (i.e. scale the number of containers due to demand), this is also a blocking factor as this means that you either need additional EC2 instances on standby (increased cost), or spin up a new instance in accordance to demand (slower scale time). You would have to accurately map and plan your port usage on your EC2 hosts such that you can launch your containers without port conflicts, which increases management overhead.\n  </p>\n\n    <h2 id=\"enter-dynamic-port-mapping\">\n      Enter Dynamic Port Mapping\n    </h2>\n\n  <p>\n    If you are going to be using multiple containers to host a service, you would also need to configure a suitable and robust solution in order to access each of these containers. When considering HTTP requests, a popular choice in AWS is to use the Application Load Balancer (ALB), often called a Layer 7 load balancer (referring to layer 7 Application layer in the <a href=\"https://www.cloudflare.com/learning/ddos/glossary/open-systems-interconnection-model-osi/\" target=\"_blank\">OSI model</a>, hence the name). An ALB will allow you to effectively distribute (or balance) requests to multiple targets to which it is connected to. While the port on the ALB to which you will issue requests to will always remain the same (port 80 for HTTP requests, and port 443 for HTTPS requests), the back end targets can each be listening on a different port, for example one target can be listening on port 8080, another can be listening on port 8000, etc. Multiple targets can even belong on the same EC2 instance, provided that the ports are different.\n  </p>\n\n  <p>\n    Using this functionality, we can apply this behaviour to containers. We can run the containers without specifying the port mapping (i.e. allow Docker to map the container port to an ephemeral port on the host), and then configure these containers to be targets for the ALB. This would allow you to run multiple containers of the same type, on the same EC2 instance, effectively reducing cost and management overhead. The ECS service is also capable of determining which host port your container has been bound to, and can configure the target registration for that specific port and container on the ALB, which allows for a seamless configuration.\n  </p>\n\n    <h2 id=\"setting-up-dynamic-port-mapping\">\n      Setting up Dynamic Port Mapping\n    </h2>\n\n  <p>\n    Dynamic Port Mapping is relatively easy to configure. From the ECS service side, all you need to do is set the \"Host\" port in the container mapping to \"0\". A JSON snippet of the task definition would therefore look like this:\n  </p>\n<pre class=\"line-numbers  language-json\"><code>\"portMappings\": [\n        {\n          \"hostPort\": 0,\n          \"protocol\": \"tcp\",\n          \"containerPort\": 80\n        }\n      ]</code></pre>\n\n  <p>\n    However, additional configurations may need to be done on your VPC and container instances in order to allow the connections to successfully reach the container. As the containers will be mapped to the ephemeral ports of the host, you would need to ensure the following criteria is met:\n  </p>\n\n  <ol>\n    <li>The security groups attached to your container instances or tasks allow for incoming connections on the ephemeral port range from the ALB nodes.</li><li>The NACLs used by the subnet in which your tasks or container instances are located must allow inbound and outbound connections on the ephemeral port range.</li>\n  </ol>\n\n  <p>\n    The ephemeral port range used by Docker versions 1.6.0 and later will refer to the&nbsp;<code>/proc/sys/net/ipv4/ip_local_port_range</code> kernel parameter on the host. If this kernel parameter is not available, or if you are using an earlier version of Docker, the default ephemeral port range is used, which ranges from ports&nbsp;49153 through 65535. In general,&nbsp;ports below 32768 fall outside of the ephemeral port range. As a general rule of thumb, you can allow incoming connections from ports 32768 through 65535 in order to account for both scenarios and avoid any issues.\n  </p>\n\n    <h2 id=\"considerations\">\n      Considerations\n    </h2>\n\n  <p>\n    As you can now have multiple containers running on a single host to which you can connect to using your ALB, a logical consideration is having too many containers on the same host. This can cause resource contention, and can cause containers to be stopped if the configurations are too restrictive. You should actively configure task and container memory and CPU limits so that each container will be given ample resources on the host in order to function effectively.\n  </p>",
            "image": "https://devcloud.co.za/media/posts/9/alb-arch.png",
            "author": {
                "name": "Jenade Moodley"
            },
            "tags": [
                   "ECS",
                   "AWS",
                   "ALB"
            ],
            "date_published": "2020-08-12T10:43:35+02:00",
            "date_modified": "2020-08-12T10:43:35+02:00"
        },
        {
            "id": "https://devcloud.co.za/about-this-website/",
            "url": "https://devcloud.co.za/about-this-website/",
            "title": "About this Website",
            "summary": " This website is a personal project which I have built as a way of sharing my experiences with various technologies, and also allows me to document said experiences for self-reference. It is also a way for me to experiment with serverless architecture; the website&hellip;",
            "content_html": "\n  <p>\n    This website is a personal project which I have built as a way of sharing my experiences with various technologies, and also allows me to document said experiences for self-reference. It is also a way for me to experiment with <a href=\"https://www.cloudflare.com/learning/serverless/what-is-serverless/\" target=\"_blank\">serverless</a> architecture; the website is a static website built using <a href=\"https://getpublii.com/\" target=\"_blank\">Publii CMS</a> (Content Management System), and is hosted on <a href=\"https://pages.github.com/\" target=\"_blank\">GitHub Pages</a>. As inquisitive users may have already gathered, DNS and SSL is managed by <a href=\"https://www.cloudflare.com/\" target=\"_blank\">CloudFlare</a>, and the site makes use of CloudFlare's CDN (Content Distribution Network) as well to boost performance.\n  </p>\n\n    <h2 id=\"looking-to-the-future\">\n      Looking to the Future\n    </h2>\n\n  <p>\n    At present, this website is basically a platform for me to share my knowledge and experiences, as well as a playground for testing serverless architecture. Any future plans for this website would depend on the reception and feedback received on the posted content. As the website is still new, no such plans have been made.\n  </p>\n\n  <p>\n    Looking at short-term goals, I will be adding more content in various categories including Linux and containers, and possibly even a coding section. I will also be adding additional AWS based content based on my experiences with certain services (such as EC2, ECS, and EFS).\n  </p>\n\n  <p>\n    And finally, feel free to share these posts if they were a worthwhile read, or if they have helped you with any issues you may have faced.&nbsp;I hope you have fun reading these posts as I have had writing them.&nbsp;\n  </p>",
            "image": "https://devcloud.co.za/media/posts/8/Optimizing-Images-for-The-Web.jpg",
            "author": {
                "name": "Jenade Moodley"
            },
            "tags": [
            ],
            "date_published": "2020-08-12T10:43:35+02:00",
            "date_modified": "2020-08-12T10:43:35+02:00"
        },
        {
            "id": "https://devcloud.co.za/improving-efs-performance-part-2/",
            "url": "https://devcloud.co.za/improving-efs-performance-part-2/",
            "title": "Improving EFS Performance (Part 2)",
            "summary": " As discussed in the part 1 of this blog post, the latency which occurs with EFS is per-file operation latency due to the nature of the NFS protocol, specifically with handling metadata. In order to improve performance of your application, you need to understand&hellip;",
            "content_html": "\n  <p>\n    As discussed in the <a href=\"https://devcloud.co.za/improving-efs-performance-part-1/\">part 1</a> of this blog post, the latency which occurs with EFS is per-file operation latency due to the nature of the NFS protocol, specifically with handling metadata. In order to improve performance of your application, you need to understand the nature of your application, i.e. is your application performing mainly read or write operations. There are a few ways to determine this, which is discussed in the NFS troubleshooting blog post, however an easy way to determine the activity of your EFS on your EC2 instance is to use the <code>mountstats</code> command.\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>$ mountstats /mnt/efs\nStats for 127.0.0.1:/ mounted on /mnt/efs:\n...\n\nNFS byte counts:\n  applications read 404148 bytes via read(2)\n  applications wrote 1966484148 bytes via write(2)\n  applications read 0 bytes via O_DIRECT read(2)\n  applications wrote 0 bytes via O_DIRECT write(2)\n  client read 0 bytes via NFS READ\n  client wrote 1966484148 bytes via NFS WRITE\n\nRPC statistics:\n  599130 RPC requests sent, 599130 RPC replies received (0 XIDs not found)\n  average backlog queue length: 0\n\nGETATTR:\n        126739 ops (21%)\n        avg bytes sent per op: 313      avg bytes received per op: 240\n        backlog wait: 0.005097  RTT: 1.268323   total execute time: 1.311964 (milliseconds)\nSETATTR:\n        122786 ops (20%)\n        avg bytes sent per op: 360      avg bytes received per op: 260\n        backlog wait: 0.004838  RTT: 4.192335   total execute time: 4.237853 (milliseconds)\nLOOKUP:\n        115717 ops (19%)\n        avg bytes sent per op: 364      avg bytes received per op: 268\n        backlog wait: 0.023964  RTT: 2.415324   total execute time: 2.497403 (milliseconds)\nACCESS:\n        72755 ops (12%)\n        avg bytes sent per op: 320      avg bytes received per op: 168\n        backlog wait: 0.008604  RTT: 1.881768   total execute time: 1.959151 (milliseconds)\nREMOVE:\n        40000 ops (6%)\n        avg bytes sent per op: 346      avg bytes received per op: 116\n        backlog wait: 0.003750  RTT: 7.256800   total execute time: 7.280375 (milliseconds)\nOPEN:\n        30225 ops (5%)\n        avg bytes sent per op: 455      avg bytes received per op: 441\n        backlog wait: 0.016973  RTT: 5.637519   total execute time: 5.663623 (milliseconds)\nCLOSE:\n        30006 ops (5%)\n        avg bytes sent per op: 336      avg bytes received per op: 176\n        backlog wait: 0.010231  RTT: 0.626241   total execute time: 0.643738 (milliseconds)\nWRITE:\n        30002 ops (5%)\n        avg bytes sent per op: 65893    avg bytes received per op: 176\n        backlog wait: 0.012932  RTT: 18.157989  total execute time: 18.181355 (milliseconds)\nRENAME:\n        30000 ops (5%)\n        avg bytes sent per op: 528      avg bytes received per op: 152\n        backlog wait: 0.007867  RTT: 5.331867   total execute time: 5.347367 (milliseconds)\nREADDIR:\n        867 ops (0%)\n        avg bytes sent per op: 342      avg bytes received per op: 32059\n        backlog wait: 0.003460  RTT: 6.455594   total execute time: 6.489043 (milliseconds)\nOPEN_NOATTR:\n        5 ops (0%)\n        avg bytes sent per op: 389      avg bytes received per op: 275\n        backlog wait: 0.000000  RTT: 2.800000   total execute time: 2.800000 (milliseconds)\nSERVER_CAPS:\n        3 ops (0%)\n        avg bytes sent per op: 316      avg bytes received per op: 164\n        backlog wait: 0.000000  RTT: 0.333333   total execute time: 0.333333 (milliseconds)\nFSINFO:\n        2 ops (0%)\n        avg bytes sent per op: 312      avg bytes received per op: 152\n        backlog wait: 0.000000  RTT: 0.500000   total execute time: 0.500000 (milliseconds)\nLOOKUP_ROOT:\n        1 ops (0%)\n        avg bytes sent per op: 184      avg bytes received per op: 380\n        backlog wait: 0.000000  RTT: 0.000000   total execute time: 0.000000 (milliseconds)\nPATHCONF:\n        1 ops (0%)\n        avg bytes sent per op: 308      avg bytes received per op: 116\n        backlog wait: 0.000000  RTT: 0.000000   total execute time: 0.000000 (milliseconds)\nSECINFO_NO_NAME:\n        1 ops (0%)\n        avg bytes sent per op: 172      avg bytes received per op: 104\n        backlog wait: 0.000000  RTT: 5.000000   total execute time: 6.000000 (milliseconds)\n</code></pre>\n\n  <p>\n    As you can see, my EFS is performing write heavy operations, and you can even see the RPC calls made most often to the EFS, the majority of which is <code>GETATTR</code> and <code>SETATTR</code>. These are for setting metadata attributes, indicating that my workload is also metadata based.\n  </p>\n\n  <p>\n    Depending on the type or workload you have, i.e. read intensive or write intensive workloads, there are different options you can configure in order to enhance the performance of your application using EFS.\n  </p>\n\n    <h2 id=\"improving-read-operation-performance\">\n      Improving read operation performance\n    </h2>\n\n  <p>\n    As we discussed, latency experienced on EFS is due to metadata access sequence of the NFS protocol. We cannot change how files are accessed from the EFS, however, we know that the local disk used is faster than EFS. Therefore, if we want to improve access time for files, we can cache the files on the local disk, effectively reducing latency by taking the NFS protocol out of the equation. This can be done using a file system cache (or fs-cache), which was created for this very purpose. A popular file system cache we can use is <code>cachefilesd</code>. To show the effects of using a cache, we can perform a test in which we access a file before and after using the cache to observe the performance improvements.\n  </p>\n\n    <h4 id=\"setting-up-cachefilesd\">\n      Setting up cachefilesd\n    </h4>\n\n  <p>\n    We first need to install <code>cachefilesd</code> using the standard package manager of your distribution.\n  </p>\n\n    <h6 id=\"amazon-linux-centos-rhel\">\n      Amazon Linux, CentOS, RHEL\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>$ sudo yum install cachefilesd -y</code></pre>\n\n    <h6 id=\"ubuntu\">\n      Ubuntu\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>$ sudo apt-get update\n$ sudo apt-get install cachefilesd</code></pre>\n\n  <p>\n    After installing the package, we need to start the <code>cachefilesd</code> daemon.\n  </p>\n\n    <h6 id=\"systemd\">\n      Systemd\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>$ sudo systemctl start cachefilesd</code></pre>\n\n    <h6 id=\"upstart\">\n      Upstart\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>$ sudo service cachefilesd start</code></pre>\n\n  <p>\n    We then need to instruct the NFS client to use the cache when accessing the EFS by specifying the <code>fsc</code> mount option when mounting the EFS.\n  </p>\n\n    <h6 id=\"using-the-efs-mount-helper\">\n      Using the EFS Mount Helper\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>$ sudo mount -t efs -o fsc &lt;EFS_ID&gt; &lt;MOUNT_POINT&gt;</code></pre>\n\n    <h6 id=\"using-the-nfs-client\">\n      Using the NFS client\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>$ sudo mount -t nfs4 -o fsc,nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport &lt;EFS_DNS&gt;:/ &lt;MOUNT_POINT&gt;</code></pre>\n\n    <h4 id=\"testing-the-cache\">\n      Testing the cache\n    </h4>\n\n  <p>\n    To test the cache, we first need to create a small file on the EFS which we will be accessing.\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>$ dd if=/dev/urandom of=/mnt/efs/cache_file bs=4k count=128</code></pre>\n\n  <p>\n    To ensure that the cache is not in effect, we can first clear the pagecache, dentries and inodes using&nbsp;<code>/proc/sys/vm/drop_caches</code>\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>$ sudo echo 3 | sudo tee /proc/sys/vm/drop_caches && sudo sync</code></pre>\n\n  <p>\n    We can then test an uncached access to the file we initially created.\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>$ time dd if=/mnt/efs/cache_file of=$HOME/local_file bs=4k count=128\n128+0 records in\n128+0 records out\n524288 bytes (524 kB) copied, 0.0176215 s, 29.8 MB/s\n\nreal    0m0.027s\nuser    0m0.002s\nsys     0m0.000s</code></pre>\n\n  <p>\n    First time accessing the file from the EFS took 0.027 seconds, at a speed of 29.8 MB/s. If we access this file again, this file will be served from the cache.\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>$ time dd if=/mnt/efs/cache_file of=$HOME/local_file bs=4k count=128\n128+0 records in\n128+0 records out\n524288 bytes (524 kB) copied, 0.00155109 s, 338 MB/s\n\nreal    0m0.009s\nuser    0m0.002s\nsys     0m0.000s</code></pre>\n\n  <p>\n    The time taken to access the file was 0.009 seconds at a speed of 338 MB/s. This is 3 times faster than the initial access time. While the access time may seem small, as the latency occurs per-file operation, this time will drastically increase for larger number of files in deeper directories.\n  </p>\n\n    <h4 id=\"limitations-and-caveats\">\n      Limitations and Caveats\n    </h4>\n\n  <p>\n    By definition, the operating system will check the cache first before accessing the EFS. This means that, should a file not exist, the entire cache will be checked before accessing the EFS. This can induce performance issues if the cache is very large and new files are being accessed more frequently than older files. This can be mitigated by configuring more options on cachefilesd such as cache culling, which&nbsp;involves discarding objects from the cache that have been used less recently than other objects. This is explained further in the <code>cachefilesd</code> <a href=\"https://linux.die.net/man/5/cachefilesd.conf\" target=\"_blank\">man pages</a>.\n  </p>\n\n    <h2 id=\"improving-write-operation-performance\">\n      Improving write operation performance\n    </h2>\n\n  <p>\n    While using <code>cachefilesd</code> can speed up access to an existing file, it does not help much for write operations as the file may not yet exist (in the case of new files), and the NFS protocol would still need to access the metadata in order to perform updates such as updating the last modified time, and update the block size. In order to get around this limitation, we can take advantage of the resilient nature of EFS.&nbsp;\n  </p>\n\n  <p>\n    EFS is designed to be accessed by thousands of clients simultaneously, and emphasis is placed on throughput of data written to the file system from all clients. This means that EFS is more situated for parallel workloads, or rather it is with parallel workloads that EFS really shines. As such, we can improve write performance by performing our commands in parallel. This can be done using two methods:\n  </p>\n\n  <ol>\n    <li>Use multiple clients (i.e. use multiple EC2 instances).&nbsp;</li><li>Use a tool such as GNU parallel or <code>fpsync</code> to run the commands in parallel.</li>\n  </ol>\n\n  <p>\n    The first option is the more recommended option as it may be difficult to rebuild your application to execute operations in parallel. However, for regular commands run on multiple files such as <code>cp</code>, <code>mv</code>, or <code>rsync</code> would benefit greatly if configured to operate on files in parallel. By default, these commands perform serial operations, i.e. it would perform the action on one file at a time. Performing the operations on multiple files simultaneously with a single command would greatly improve the performance. To demonstrate the results, as well as the ease of using a parallel utility, we can perform the below test using GNU parallel.&nbsp;<br>\n  </p>\n\n    <h4 id=\"installing-gnu-parallel\">\n      Installing GNU Parallel\n    </h4>\n\n  <p>\n    The commands to install GNU parallel would differ per distribution. For RPM-based distributions (RedHat, CentOS, Amazon Linux), it is available in the <code>epel</code> repository. For Ubuntu, it is available in the default repository. The commands to install GNU parallel would be as below:\n  </p>\n\n    <h6 id=\"rhel-7\">\n      RHEL 7\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>$ sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n$ sudo yum install parallel -y</code></pre>\n\n    <h6 id=\"amazon-linux-rhel-6\">\n      Amazon Linux, RHEL 6\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>$ sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm\n$ sudo yum install parallel -y</code></pre>\n\n    <h6 id=\"amazon-linux-2\">\n      Amazon Linux 2\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>$ sudo amazon-linux-extras install epel\n$ sudo yum install parallel -y\n</code></pre>\n\n    <h6 id=\"centos-6-centos-7\">\n      CentOS 6, CentOS 7\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>$ sudo yum install epel-release\n$ sudo yum install parallel -y\n</code></pre>\n\n    <h6 id=\"ubuntu\">\n      Ubuntu\n    </h6>\n<pre class=\"line-numbers  language-bash\"><code>$ sudo apt-get update\n$ sudo apt-get install parallel</code></pre>\n\n    <h4 id=\"running-commands-in-parallel\">\n      Running commands in parallel\n    </h4>\n\n  <p>\n    We first need to create a large number of small files on our local disk. In my test, I have created 10000 files. For reference, I am using a standard <code>t2.large</code>&nbsp;EC2 instance running Amazon Linux 2, with a general purpose (gp2) volume of 8 GB in size.\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>$ mkdir /tmp/efs; for each in $(seq 1 10000); do SUFFIX=$(mktemp -u _XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX); sudo dd if=/dev/zero of=/tmp/efs/${SUFFIX} bs=64k count=1; done</code></pre>\n\n  <p>\n    We can then copy these files to the EFS with <code>rsync</code> using the standard serial method, and measure the time it takes to copy the files.\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>$ cd /tmp/efs; time sudo find -L . -maxdepth 1 -type f -exec rsync -avR '{}' /mnt/efs/ \\;\n...\nreal    9m11.494s\nuser    0m34.186s\nsys     0m10.303s</code></pre>\n\n  <p>\n    The results show that it took longer than 9 minutes to copy 10000 files to the EFS using <code>rsync</code>. If we purge the directory, and use <code>rsync</code> in parallel:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>$ cd /tmp/efs; time find -L . -maxdepth 1 -type f | sudo parallel rsync -avR {} /mnt/efs/\n...\nreal    5m41.274s\nuser    1m1.279s\nsys     0m37.556s\n</code></pre>\n\n  <p>\n    The files took longer than 5 minutes, effectively reducing the copy time by almost half. Furthermore, GNU parallel runs as many parallel jobs as there are CPU cores. As the <code>t2.large</code> instance contains 2 cores, this explains why the time was almost halved. However, GNU parallel allows you to specify the number of jobs by using the<code> --jobs</code> flag. If we set the <code>--jobs</code> flag to 0, it would try to run as many jobs as possible with the amount of CPU available.\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>$ cd /tmp/efs; time find -L . -maxdepth 1 -type f | sudo parallel --jobs 0 rsync -avR {} /mnt/efs/\n...\nreal    3m25.578s\nuser    1m2.320s\nsys     0m45.163s\n</code></pre>\n\n  <p>\n    Using as many jobs as possible, we reduced the time taken to about a third of the time it would have taken when using the serial copy method. You can also edit and configure GNU parallel commands as your needs. You can find a more detailed tutorial of GNU parallel <a href=\"https://www.gnu.org/software/parallel/parallel_tutorial.html#GNU-Parallel-Tutorial\">here</a>.\n  </p>\n\n    <h2 id=\"conclusion\">\n      Conclusion\n    </h2>\n\n  <p>\n    Knowing your application's usage patterns can greatly assist you with configuring your EFS. Using the above suggestions correctly can help you to use the EFS to it's full potential, however incorrect use of these suggestions, such as implementing <code>cachefilesd</code> for an application which only accesses recent data on the EFS, can cause further performance degradation. There are additional caching options you can look at such as <a href=\"https://www.php.net/manual/en/book.opcache.php\" target=\"_blank\">PHP-opcache</a> for PHP-based websites, and incorporating a CDN such as <a href=\"https://aws.amazon.com/cloudfront/\" target=\"_blank\">CloudFront</a>, however these are website specific caching mechanisms. You need to identify a good mix of services and caching mechanisms in order to improve your application performance on EFS, and I hope this blog post gives you a good starting point on improving your experience with EFS.\n  </p>",
            "image": "https://devcloud.co.za/media/posts/7/AWS_EFS-system_DIMq8h4.jpg",
            "author": {
                "name": "Jenade Moodley"
            },
            "tags": [
                   "NFS",
                   "EFS",
                   "AWS"
            ],
            "date_published": "2020-08-12T10:43:35+02:00",
            "date_modified": "2020-08-12T10:43:35+02:00"
        },
        {
            "id": "https://devcloud.co.za/improving-efs-performance-part-1/",
            "url": "https://devcloud.co.za/improving-efs-performance-part-1/",
            "title": "Improving EFS Performance (Part 1)",
            "summary": " Why is my EFS slow? If you have started using EFS as the main root storage for your application such as your WordPress website, or a common storage layer for your containerized applications, you may notice that the performance obtained when using the EFS&hellip;",
            "content_html": "\n    <h2 id=\"why-is-my-efs-slow\">\n      Why is my EFS slow?\n    </h2>\n\n  <p>\n    If you have started using EFS as the main root storage for your application such as your WordPress website, or a common storage layer for your containerized applications, you may notice that the performance obtained when using the EFS appears to be slower than using regular disks. This slowness is not caused deliberately, rather it is due to the nature of EFS.&nbsp;\n  </p>\n\n  <p>\n    Using EFS without being aware of it's architecture and limitations can catch you unawares, however, by understanding how EFS works and the nature of your application, it is possible to boost the performance of your application using EFS (notice I said application performance and not EFS performance.)\n  </p>\n\n    <h2 id=\"efs-architecture\">\n      EFS Architecture\n    </h2>\n\n  <p>\n    Straight from AWS' documentation, EFS is a distributed file system which is designed to be accessed by thousands of clients simultaneously, provide redundant and highly available storage across multiple availability zones, and provide low latency per-file operation. The latter of these claims otherwise towards the slowness experienced, however the key-words to remember is \"per-file operation\". It is an NFS file system, and is designed to be accessed via the NFS protocol. Supported versions of the protocol are NFS version 4, and NFS version 4.1 (preferred).\n  </p>\n\n  <p>\n    EFS manages all file storage infrastructure for you, and you only pay for the storage that you use. You do not have to provision any servers or instances, nor do you have to manage any storage or disks. It also guarantees \"close-to-open\" data consistency and supports POSIX permissions.\n  </p>\n\n    <h2 id=\"understanding-perfile-operation-latencynbsp\">\n      Understanding \"per-file operation\" latency&nbsp;\n    </h2>\n\n  <p>\n    To understand why per-file operation latency can induce slowness, we first need to understand how the operating system works with files in general. EFS, at present, can only be accessed by Linux-based clients, and every single component of Linux, be it an application or configuration, consists of files. In short, every action on your Linux operating system requires accessing a file.\n  </p>\n\n  <p>\n    Unpacking that analogy, there are only two options which are applicable to the use of files; reading, and writing to a file. From the operating system's point of view, either of these actions is interpreted as an \"operation\", and there are appropriate read and write operations available as the operating system would need to perform different actions for these operations. This is unpacked further here (coming soon).\n  </p>\n\n  <p>\n    When reading or writing to a file, the operating system needs to first obtain the metadata for the file, which contains various information about the file including the inode of the file, the permissions to determine if the file is accessible by the user or application performing the operation, as well as the block size and IO block (location on the disk) for the actual data on the file. If the user or application has sufficient permissions to access the file, the operating system would then access the contents of the file from the disk.\n  </p>\n\n  <p>\n    Coming back to EFS, we know that EFS uses the NFS protocol to run file operations. What you may not know, is that NFS is not too efficient with handling metadata, which is necessary for all file operations. To briefly explain, your file would be located within a directory, such as the root EFS directory or several directories within the EFS. When trying to access a file, the NFS protocol will recursively check each directory to determine if the user has sufficient permissions to access each directory, before finally arriving at the specified file. As you are connecting to the EFS over a network, each of these checks is a Remote Procedure Call (RPC). As these checks occur over a network, while these checks occur very fast, they are much slower when compared to directly attached storage options. A deeper overview of the NFS protocol sequence is explained <a href=\"http://www.eventhelix.com/realtimemantra/networking/NFS_Protocol_Sequence_Diagram.pdf\" target=\"_blank\">here</a>.\n  </p>\n\n    <h2 id=\"whats-next\">\n      What's next?\n    </h2>\n\n  <p>\n    Now that we understand what causes latency with the EFS, we can go ahead and start looking methods for improving the performance of the applications which use the EFS. This is discussed in the <a href=\"https://devcloud.co.za/improving-efs-performance-part-2/\">second part of this blog post</a>.\n  </p>",
            "image": "https://devcloud.co.za/media/posts/6/AWS_EFS-system_DIMq8h4.jpg",
            "author": {
                "name": "Jenade Moodley"
            },
            "tags": [
                   "NFS",
                   "EFS",
                   "AWS"
            ],
            "date_published": "2020-08-12T10:43:35+02:00",
            "date_modified": "2020-08-12T10:43:35+02:00"
        }
    ]
}
